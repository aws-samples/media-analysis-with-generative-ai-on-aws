"""
Utility functions for prompt caching and windowing demos
Keeps demo cells focused on core Bedrock + caching concepts
"""

import json


def load_system_prompt(prompt_file='prompts/video_analysis_system_prompt.txt'):
    """Load system prompt from file"""
    with open(prompt_file, 'r') as f:
        return f.read()


def print_payload_structure(request_body, call_number, max_text_len=80):
    """Print JSON payload with cache breakpoints highlighted"""
    print(f"\n{'='*100}")
    print(f"üì§ API CALL #{call_number} - REQUEST PAYLOAD")
    print(f"{'='*100}")
    print("\nüîç Payload Structure (cache breakpoints marked with ‚ö°):\n")
    _print_json_recursive(request_body, indent=0, max_text_len=max_text_len)
    print(f"\n{'='*100}\n")


def _print_json_recursive(obj, indent=0, max_text_len=80):
    """Recursively print JSON with cache control highlighted"""
    spaces = "  " * indent
    
    if isinstance(obj, dict):
        print(f"{spaces}{{")
        for i, (key, value) in enumerate(obj.items()):
            comma = "," if i < len(obj) - 1 else ""
            
            if key == "cache_control":
                # Highlight cache breakpoints
                print(f"{spaces}  \"{key}\": ", end="")
                print(f"\033[93m{json.dumps(value)}\033[0m {comma}  ‚ö° CACHE BREAKPOINT")
            elif key == "data" and isinstance(value, str) and len(value) > max_text_len:
                print(f"{spaces}  \"{key}\": \"<base64_image_{len(value)}_chars>\"{comma}")
            elif key == "text" and isinstance(value, str) and len(value) > max_text_len:
                truncated = value[:max_text_len] + "... [TRUNCATED]"
                print(f"{spaces}  \"{key}\": \"{truncated}\"{comma}")
            elif isinstance(value, (dict, list)):
                print(f"{spaces}  \"{key}\": ", end="")
                if isinstance(value, list) and len(value) == 0:
                    print(f"[]{comma}")
                else:
                    print()
                    _print_json_recursive(value, indent + 1, max_text_len)
                    print(f"{comma}")
            else:
                print(f"{spaces}  \"{key}\": {json.dumps(value)}{comma}")
        print(f"{spaces}}}", end="")
    
    elif isinstance(obj, list):
        if len(obj) == 0:
            print(f"{spaces}[]", end="")
        else:
            print(f"{spaces}[")
            for i, item in enumerate(obj):
                comma = "," if i < len(obj) - 1 else ""
                _print_json_recursive(item, indent + 1, max_text_len)
                print(comma)
            print(f"{spaces}]", end="")
    else:
        print(f"{spaces}{json.dumps(obj)}", end="")


def print_token_metrics(usage, call_number, call_duration):
    """Print token usage and cache performance metrics"""
    input_tokens = usage.get('input_tokens', 0)
    output_tokens = usage.get('output_tokens', 0)
    cache_read = usage.get('cache_read_input_tokens', 0)
    cache_write = usage.get('cache_creation_input_tokens', 0)
    
    total_input = input_tokens + cache_read
    cache_hit_ratio = (cache_read / total_input * 100) if total_input > 0 else 0
    
    print(f"{'='*100}")
    print(f"üìä CALL #{call_number} - TOKEN USAGE & CACHE PERFORMANCE")
    print(f"{'='*100}")
    print(f"\n‚è±Ô∏è  API Call Duration: {call_duration:.2f}s")
    print(f"\nüì• INPUT TOKENS:")
    print(f"   ‚Ä¢ Regular input:  {input_tokens:>8,} tokens")
    print(f"   ‚Ä¢ Cache read:     {cache_read:>8,} tokens (90% cheaper) {'‚úÖ' if cache_read > 0 else '‚ùÑÔ∏è'}")
    print(f"   ‚Ä¢ Cache write:    {cache_write:>8,} tokens (25% more expensive)")
    print(f"   ‚Ä¢ Total input:    {total_input:>8,} tokens")
    
    print(f"\nüì§ OUTPUT TOKENS:")
    print(f"   ‚Ä¢ Generated:      {output_tokens:>8,} tokens")
    
    print(f"\nüíæ CACHE PERFORMANCE:")
    print(f"   ‚Ä¢ Hit ratio:      {cache_hit_ratio:>7.1f}%")
    if cache_read > 0:
        print(f"   ‚Ä¢ Status:         ‚úÖ Cache working!")
    else:
        print(f"   ‚Ä¢ Status:         ‚ùÑÔ∏è  Cold start (no cache)")
    
    print(f"\n{'='*100}\n")
    
    return {
        'call_number': call_number,
        'input_tokens': input_tokens,
        'output_tokens': output_tokens,
        'cache_read': cache_read,
        'cache_write': cache_write,
        'cache_hit_ratio': cache_hit_ratio,
        'duration': call_duration
    }


def print_summary_table(metrics_list):
    """Print cumulative summary table"""
    print(f"\n{'='*100}")
    print(f"üìà CUMULATIVE SUMMARY - ALL API CALLS")
    print(f"{'='*100}")
    print()
    print("üìä Column Definitions:")
    print("   ‚Ä¢ Input: Regular input tokens (new content, not from cache)")
    print("   ‚Ä¢ Output: Tokens generated by the model")
    print("   ‚Ä¢ Cache R: Tokens read from cache (90% cheaper)")
    print("   ‚Ä¢ Cache W: Tokens written to cache (25% premium, one-time)")
    print("   ‚Ä¢ Hit %: Percentage of total input served from cache")
    print("   ‚Ä¢ Duration: API call time in seconds")
    print()
    
    # Calculate totals
    total_input = sum(m['input_tokens'] for m in metrics_list)
    total_output = sum(m['output_tokens'] for m in metrics_list)
    total_cache_read = sum(m['cache_read'] for m in metrics_list)
    total_cache_write = sum(m['cache_write'] for m in metrics_list)
    total_duration = sum(m['duration'] for m in metrics_list)
    
    overall_cache_hit = (total_cache_read / (total_input + total_cache_read) * 100) if (total_input + total_cache_read) > 0 else 0
    
    # Print table
    print(f"{'Call':<6} {'Input':<10} {'Output':<10} {'Cache R':<10} {'Cache W':<10} {'Hit %':<8} {'Duration':<10}")
    print(f"{'-'*86}")
    
    for m in metrics_list:
        print(f"#{m['call_number']:<5} "
              f"{m['input_tokens']:<10,} "
              f"{m['output_tokens']:<10,} "
              f"{m['cache_read']:<10,} "
              f"{m['cache_write']:<10,} "
              f"{m['cache_hit_ratio']:<7.1f}% "
              f"{m['duration']:<9.2f}s")
    
    print(f"{'-'*86}")
    print(f"{'TOTAL':<6} "
          f"{total_input:<10,} "
          f"{total_output:<10,} "
          f"{total_cache_read:<10,} "
          f"{total_cache_write:<10,} "
          f"{overall_cache_hit:<7.1f}% "
          f"{total_duration:<9.2f}s")
    
    print(f"\nüìä CACHE EFFICIENCY:")
    print(f"   ‚Ä¢ Overall cache hit ratio:    {overall_cache_hit:.1f}%")
    print(f"   ‚Ä¢ Total tokens processed:     {total_input + total_output + total_cache_read + total_cache_write:,}")
    print(f"   ‚Ä¢ Total tokens cached (read): {total_cache_read:,}")
    
    print(f"\n{'='*100}\n")


def print_first_call_explanation():
    """Print explanation after first call"""
    print("\n" + "üí°" * 50)
    print("üìö UNDERSTANDING CALL #1 METRICS:")
    print("   ‚Ä¢ Cache Write > Regular Input is NORMAL for first call")
    print("   ‚Ä¢ Cache Write includes: System Prompt (~1,200 tokens) + Current Request (~1,600 tokens)")
    print("   ‚Ä¢ Regular Input: Only the new content being processed")
    print("   ‚Ä¢ This upfront cost (25% premium) enables 90% savings on future calls!")
    print("üí°" * 50 + "\n")


# ============================================================================
# Smart Context Windowing Demo Utilities
# ============================================================================

def print_windowing_comparison_table(metrics_no_window, metrics_with_window, n_chapters):
    """Print comparison table for windowing demo"""
    print("\n" + "=" * 100)
    print("üìä COMPARISON: Context Size Over Time")
    print("=" * 100)
    print()
    print("üìñ Column Definitions:")
    print("   ‚Ä¢ Messages: Number of conversation messages in context (user + assistant pairs)")
    print("   ‚Ä¢ Tokens: Estimated tokens in conversation history (~4 chars = 1 token)")
    print("   ‚Ä¢ Chapters: Total chapters in context (finalized + current incomplete chapter)")
    print()
    print("üí° Important Notes:")
    print("   ‚Ä¢ Each row shows context AFTER processing that chunk")
    print("   ‚Ä¢ Chapters = ALL chapters in analysis state (finalized + current being built)")
    print("   ‚Ä¢ With keep_n_chapters=0: Stays at 1 (only current chapter, finalized ones removed)")
    print("   ‚Ä¢ With keep_n_chapters=1: Bounded at 2 (1 finalized + 1 current)")
    print("   ‚Ä¢ With keep_n_chapters=None: Grows unbounded (all chapters kept)")
    print()
    print(f"{'Chunk':<8} {'Without Windowing':<40} {'With Windowing (n={n_chapters})':<40}")
    print(f"{'':8} {'Messages':<12} {'Tokens':<12} {'Chapters':<12} {'Messages':<12} {'Tokens':<12} {'Chapters':<12}")
    print("-" * 100)
    
    num_chunks = len(metrics_no_window)
    for i in range(num_chunks):
        no_win = metrics_no_window[i]
        with_win = metrics_with_window[i]
        
        print(f"#{i:<7} "
              f"{no_win['num_messages']:<12} "
              f"{no_win['context_tokens']:<12,} "
              f"{no_win['chapters_in_context']:<12} "
              f"{with_win['num_messages']:<12} "
              f"{with_win['context_tokens']:<12,} "
              f"{with_win['chapters_in_context']:<12}")
    
    # Calculate savings
    total_no_win = sum(m['context_tokens'] for m in metrics_no_window)
    total_with_win = sum(m['context_tokens'] for m in metrics_with_window)
    savings = total_no_win - total_with_win
    savings_pct = (savings / total_no_win * 100) if total_no_win > 0 else 0
    
    print("-" * 100)
    print(f"{'TOTAL':<8} "
          f"{'':12} "
          f"{total_no_win:<12,} "
          f"{'':12} "
          f"{'':12} "
          f"{total_with_win:<12,} "
          f"{'':12}")
    
    print()
    print("üí∞ TOKEN SAVINGS:")
    print(f"   ‚Ä¢ Without windowing: {total_no_win:,} context tokens")
    print(f"   ‚Ä¢ With windowing:    {total_with_win:,} context tokens")
    print(f"   ‚Ä¢ Savings:           {savings:,} tokens ({savings_pct:.1f}%)")
    
    print()
    print("üìà KEY OBSERVATIONS:")
    print(f"   ‚Ä¢ Without windowing: Context grows from {metrics_no_window[0]['num_messages']} to {metrics_no_window[-1]['num_messages']} messages")
    print(f"   ‚Ä¢ With windowing: Context stabilizes at ~{metrics_with_window[-1]['num_messages']} messages")
    print(f"   ‚Ä¢ Token reduction: {savings_pct:.1f}% fewer tokens in context")
    print(f"   ‚Ä¢ Analysis quality: Identical (model makes same decisions)")
    
    print("\n" + "=" * 100)


def print_windowing_key_learnings(savings_pct):
    """Print key learnings for windowing demo"""
    print("\n" + "=" * 100)
    print("üéì KEY LEARNINGS")
    print("=" * 100)
    print("\n1. üìâ Bounded Context:")
    print("   ‚Ä¢ Smart windowing keeps context size constant")
    print("   ‚Ä¢ Prevents unbounded growth as video gets longer")
    print("\n2. üí∞ Cost Efficiency:")
    print(f"   ‚Ä¢ {savings_pct:.1f}% reduction in context tokens")
    print("   ‚Ä¢ Scales linearly with video length")
    print("\n3. üéØ Maintained Accuracy:")
    print("   ‚Ä¢ Model only needs recent context for decisions")
    print("   ‚Ä¢ No loss in analysis quality")
    print("\n4. üöÄ Combined Power:")
    print("   ‚Ä¢ Prompt Caching: 60-80% savings on cached content")
    print(f"   ‚Ä¢ Smart Windowing: {savings_pct:.1f}% reduction in context size")
    print("   ‚Ä¢ Together: Maximum efficiency for long videos!")
    print("=" * 100)
