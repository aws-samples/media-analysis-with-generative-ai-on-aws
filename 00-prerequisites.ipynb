{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c84a641-be01-4cd5-8b24-f175f16121c4",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9ab7a0-5fd1-4418-a0c2-4e401ad47eef",
   "metadata": {},
   "source": [
    "## Workshop overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9314b5cb-335e-48d2-b26a-a1ab92291f36",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Welcome to the Video Understanding on AWS workshop!\n",
    "\n",
    "The workshop is organized into two main parts: 1. Media Analysis using Bedrock Data Automation and 2. Media Analysis using Amazon Nova. Read on to get an overview of the different sections.  Part 1 and Part 2 can be run independently after running this notebook.\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "Before running the main workshop, you'll set up the notebook environment using this notebook.\n",
    "\n",
    "**Part 1: Media Analysis using Bedrock Data Automation (BDA):**\n",
    "\n",
    "The notebooks in this section give an overview of BDA APIs and use cases.  They can be run in any order.  \n",
    "\n",
    "1. [Extract and analyze a movie with BDA](1-media-analysis-using-bda/01-extract-analyze-a-movie.ipynb)\n",
    "2. [Contextual Ad overlay](1-media-analysis-using-bda/02-contextual-ad-overlay.ipynb)\n",
    "\n",
    "**Part 2: Media Analysis using Amazon Nova:**\n",
    "\n",
    "In the foundation notebooks, you'll set up the notebook environment, prepare the sample video by breaking it down into clips, and you will experiment with using Foundation models to generate insights about video clips.  In the second part of the workshop, you will use the foundations to solve different video understanding use cases.  The use cases are independent and can be run in any order.\n",
    "\n",
    "**Foundation (required before running use cases)**\n",
    "1. [Visual video segments: frames, shots and scenes](2-media-analysis-using-amazon-nova/01A-visual-segments-frames-shots-scenes.ipynb) (20 minutes)\n",
    "2. [Audio segments](2-media-analysis-using-amazon-nova/01B-audio-segments.ipynb) (10 minutes)\n",
    "\n",
    "**Use cases (optional, run in any order):**\n",
    "\n",
    "After running the Foundations notebooks, you can choose any use case.  If you are running at an AWS Workshop event, you will be able to complete foundations plus one use case in a 2 hour session:\n",
    "\n",
    "* [Ad break detection and contextual Ad tartgeting](2-media-analysis-using-amazon-nova/02-ad-breaks-and-contextual-ad-targeting.ipynb) (20 minutes) - identify opportunities for ad insertion.  Use a standard taxonomy to match video content to ad content.\n",
    "* [Video summarization](2-media-analysis-using-amazon-nova/03-video-summarization.ipynb) (20 minutes) - generate short form videos from a longer video\n",
    "* [Semantic video search](2-media-analysis-using-amazon-nova/04-semantic-video-search.ipynb) (20 minutes) - search video using images and natural language to find relevant clips\n",
    "\n",
    "**Resources**\n",
    "\n",
    "The activities in this workshop are based on AWS Solution Guidance.  The [Additional Resources](./09-resources.ipynb) lab contains links to relevant reference architectures, code samples and blog posts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fce212e-9c89-4b32-8327-52fae47b2ff5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Install ffmpeg and python packages\n",
    "\n",
    "- ffmpeg for video and image processing\n",
    "- faiss for vector store\n",
    "- webvtt-py for parsing subtitle file\n",
    "- termcolor for formatting output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab951b69-de93-47d8-9d1a-1ee1cbe334dc",
   "metadata": {
    "editable": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## install ffmpeg (Linux/SageMaker only)\n",
    "# On macOS, install with: brew install ffmpeg\n",
    "import platform\n",
    "if platform.system() == 'Linux':\n",
    "    !sudo apt update -y && sudo apt-get -y install ffmpeg\n",
    "else:\n",
    "    print(f\"Skipping apt install on {platform.system()}. Install ffmpeg manually if needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc9bef1-db74-4935-b033-a25e4738e7c6",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Check if ffmpeg is installed\n",
    "import shutil\n",
    "if shutil.which('ffmpeg'):\n",
    "    print(f\"✓ ffmpeg found at: {shutil.which('ffmpeg')}\")\n",
    "else:\n",
    "    print(\"✗ ffmpeg not found. Please install it:\")\n",
    "    print(\"  macOS: brew install ffmpeg\")\n",
    "    print(\"  Linux: sudo apt-get install ffmpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8a00ae-4107-4817-988e-15af6ed7ff7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7rxylnv9qss",
   "metadata": {},
   "source": [
    "## Verify Package Imports\n",
    "\n",
    "Verify that all required libraries can be imported successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bmcbv3bgh2m",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from importlib import import_module\n",
    "\n",
    "# List of required packages to verify\n",
    "required_packages = [\n",
    "    ('boto3', 'AWS SDK'),\n",
    "    ('websockets', 'Networking'),\n",
    "    ('numpy', 'Data Processing'),\n",
    "    ('pandas', 'Data Processing'),\n",
    "    ('jupyter', 'Notebook Support'),\n",
    "    ('IPython', 'Notebook Support'),\n",
    "    ('ipywidgets', 'Notebook Widgets'),\n",
    "    ('cv2', 'Video Processing (OpenCV)'),\n",
    "    ('PIL', 'Image Processing (Pillow)'),\n",
    "    ('dotenv', 'Environment Variables'),\n",
    "    ('requests', 'HTTP Requests'),\n",
    "]\n",
    "\n",
    "print(\"Verifying core packages...\")\n",
    "failed_imports = []\n",
    "for package, description in required_packages:\n",
    "    try:\n",
    "        mod = import_module(package)\n",
    "        version = getattr(mod, '__version__', 'N/A')\n",
    "        print(f\"✓ {package:20s} (v{version})\")\n",
    "    except ImportError as e:\n",
    "        failed_imports.append((package, description))\n",
    "        print(f\"✗ {package:20s} FAILED\")\n",
    "\n",
    "# Verify AWS Bedrock Agent packages\n",
    "print(\"\\nVerifying Bedrock Agent packages...\")\n",
    "bedrock_packages = ['strands', 'strands_tools', 'bedrock_agentcore']\n",
    "bedrock_failed = []\n",
    "\n",
    "for package in bedrock_packages:\n",
    "    try:\n",
    "        mod = import_module(package)\n",
    "        version = getattr(mod, '__version__', 'N/A')\n",
    "        print(f\"✓ {package:30s} (v{version})\")\n",
    "    except ImportError as e:\n",
    "        bedrock_failed.append(package)\n",
    "        print(f\"✗ {package:30s} FAILED\")\n",
    "\n",
    "# Final summary\n",
    "total_packages = len(required_packages) + len(bedrock_packages)\n",
    "total_failed = len(failed_imports) + len(bedrock_failed)\n",
    "\n",
    "if total_failed == 0:\n",
    "    print(f\"\\n✅ All {total_packages} packages imported successfully!\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ {total_failed} package(s) failed to import\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626bb1bf-f941-4b26-be79-3ca659c87639",
   "metadata": {},
   "source": [
    "## Get SageMaker default resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b8b333-7644-4665-95b5-85ce17eae42b",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sagemaker_resources = {}\n",
    "\n",
    "# Try to get SageMaker execution role, fallback to boto3 session if not in SageMaker\n",
    "try:\n",
    "    import sagemaker\n",
    "    sagemaker_resources[\"role\"] = sagemaker.get_execution_role()\n",
    "    sagemaker_resources[\"region\"] = sagemaker.Session()._region_name\n",
    "    print(\"Running in SageMaker environment\")\n",
    "except Exception:\n",
    "    # Not in SageMaker, use boto3 session\n",
    "    session = boto3.Session()\n",
    "    sagemaker_resources[\"role\"] = None  # Not needed outside SageMaker\n",
    "    sagemaker_resources[\"region\"] = session.region_name\n",
    "    print(\"Running in local/non-SageMaker environment\")\n",
    "\n",
    "print(sagemaker_resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb318ff-0b5c-4095-bddf-4eb947baf170",
   "metadata": {},
   "source": [
    "# Setup session AWS resources\n",
    "\n",
    "The cell below loads AWS resources from the CloudFormation stack outputs. This works for both:\n",
    "- AWS hosted events (using the full `workshop.yaml` stack)\n",
    "- Your own AWS account (using the minimal `workshop-customer.yaml` stack)\n",
    "\n",
    "Both stacks use the same stack name (`workshop`) and provide the same outputs, so this notebook works in either environment.\n",
    "\n",
    "### Deployment Options\n",
    "\n",
    "**Automatic deployment from notebook (recommended)**\n",
    "\n",
    "Run the code cell below. If the stack doesn't exist, it will automatically deploy `workshop-customer.yaml` with your current user/role ARN for OpenSearch access.\n",
    "\n",
    "**Note:** The `UserOrRoleArn` parameter is optional but recommended if you're running outside of SageMaker or need OpenSearch access from a specific IAM identity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a018cb6-c58c-41b5-bcdf-263ff932bba2",
   "metadata": {},
   "source": [
    "## Get CloudFormation stack outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb73ce3b-4c34-4fff-8cff-642dcd0c3352",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from IPython.display import JSON\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "cf = boto3.client(service_name=\"cloudformation\")\n",
    "sts = boto3.client(service_name=\"sts\")\n",
    "\n",
    "try:\n",
    "    stack = cf.describe_stacks(StackName='workshop')\n",
    "    print(\"✓ Stack found\")\n",
    "except ClientError:\n",
    "    print(\"Stack not found. Deploying workshop-customer.yaml...\")\n",
    "    \n",
    "    # Get current user/role ARN for OpenSearch access\n",
    "    try:\n",
    "        caller_identity = sts.get_caller_identity()\n",
    "        current_arn = caller_identity['Arn']\n",
    "        print(f\"  Detected identity: {current_arn}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not detect current ARN: {e}\")\n",
    "        current_arn = \"\"\n",
    "    \n",
    "    with open('workshop-customer.yaml', 'r') as f:\n",
    "        template_body = f.read()\n",
    "    \n",
    "    # Create stack with current user ARN for OpenSearch access\n",
    "    stack_params = {\n",
    "        'StackName': 'workshop',\n",
    "        'TemplateBody': template_body,\n",
    "        'Capabilities': ['CAPABILITY_NAMED_IAM']\n",
    "    }\n",
    "    \n",
    "    if current_arn:\n",
    "        stack_params['Parameters'] = [\n",
    "            {'ParameterKey': 'UserOrRoleArn', 'ParameterValue': current_arn}\n",
    "        ]\n",
    "    \n",
    "    cf.create_stack(**stack_params)\n",
    "    \n",
    "    print(\"  Waiting for stack creation (5-10 minutes)...\")\n",
    "    cf.get_waiter('stack_create_complete').wait(StackName='workshop')\n",
    "    \n",
    "    stack = cf.describe_stacks(StackName='workshop')\n",
    "    print(\"✓ Stack deployed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f450a-d480-45d9-a518-d90cb14027ee",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "JSON(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6771eae-5fc9-408e-8464-1ed9726b9955",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = {}\n",
    "session['bucket'] = next(item[\"OutputValue\"] for item in stack['Stacks'][0]['Outputs'] if item[\"OutputKey\"] == \"S3BucketName\")\n",
    "session['MediaConvertRole'] = next(item[\"OutputValue\"] for item in stack['Stacks'][0]['Outputs'] if item[\"OutputKey\"] == \"MediaConvertRole\")\n",
    "session[\"AOSSCollectionEndpoint\"] = next(item[\"OutputValue\"] for item in stack['Stacks'][0]['Outputs'] if item[\"OutputKey\"] == \"AOSSCollectionEndpoint\")\n",
    "\n",
    "print(\"\\nWorkshop resources loaded:\")\n",
    "print(f\"  S3 Bucket: {session['bucket']}\")\n",
    "print(f\"  MediaConvert Role: {session['MediaConvertRole']}\")\n",
    "print(f\"  OpenSearch Collection: {session['AOSSCollectionEndpoint']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v1pkhskm24g",
   "metadata": {},
   "source": [
    "## Configure Bedrock Models\n",
    "\n",
    "Configure the Amazon Bedrock foundation model IDs for visual, audio, and audiovisual understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6821814-6146-43fe-96e2-c9747f4ea45c",
   "metadata": {},
   "source": [
    "### Subscribe to Claude Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee035cf-ffb2-47ce-a65e-8155f956f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def subscribeTo3PBedrock(modelId):\n",
    "    bedrock = boto3.client('bedrock-runtime', region_name='us-west-2')\n",
    "    bedrock.invoke_model(\n",
    "        modelId=modelId,\n",
    "        body=json.dumps({\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 200,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"ping\"}]\n",
    "        })\n",
    "    )\n",
    "\n",
    "modelsUsed = ['global.anthropic.claude-sonnet-4-20250514-v1:0']\n",
    "\n",
    "for model in modelsUsed: \n",
    "    subscribeTo3PBedrock(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dstnsumbxan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "print(\"Configuring Bedrock Model IDs...\")\n",
    "\n",
    "# Visual Understanding Model (for image analysis)\n",
    "VISUAL_MODEL_ID = \"global.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "\n",
    "# Audio Understanding Model (for audio transcription analysis)\n",
    "AUDIO_MODEL_ID = \"global.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "\n",
    "# Audiovisual Understanding Model (for multimodal fusion)\n",
    "AUDIOVISUAL_MODEL_ID = \"global.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "\n",
    "AWS_REGION = sagemaker_resources[\"region\"]\n",
    "\n",
    "print(f\"✅ Visual Model: {VISUAL_MODEL_ID}\")\n",
    "print(f\"✅ Audio Model: {AUDIO_MODEL_ID}\")\n",
    "print(f\"✅ Audiovisual Model: {AUDIOVISUAL_MODEL_ID}\")\n",
    "print(f\"✅ Region: {AWS_REGION}\")\n",
    "\n",
    "# Store variables for use in other notebooks\n",
    "%store VISUAL_MODEL_ID\n",
    "%store AUDIO_MODEL_ID\n",
    "%store AUDIOVISUAL_MODEL_ID\n",
    "%store AWS_REGION\n",
    "\n",
    "print(\"\\n✅ Model configuration saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nn1mwirhd2n",
   "metadata": {},
   "source": [
    "## Configure Bedrock AgentCore Memory and Knowledge Base\n",
    "\n",
    "Deploy the live-vu-lab CloudFormation stack to create AgentCore Memory and Knowledge Base resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e508a0df-0c96-44cd-a3c8-32aacb6fab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "print(\"Configuring Bedrock AgentCore Memory and Knowledge Base...\")\n",
    "\n",
    "cfn_client = boto3.client('cloudformation', region_name=AWS_REGION)\n",
    "stack_name = 'ws-intelligent-mw-agentic-ai-lab'\n",
    "\n",
    "try:\n",
    "    # Check if stack exists\n",
    "    stack = cfn_client.describe_stacks(StackName=stack_name)\n",
    "    print(f\"✓ Stack '{stack_name}' found\")\n",
    "except ClientError:\n",
    "    print(f\"Stack '{stack_name}' not found. Deploying live-vu-lab.yaml...\")\n",
    "    \n",
    "    # Read the CloudFormation template\n",
    "    with open('live-vu-lab.yaml', 'r') as f:\n",
    "        template_body = f.read()\n",
    "    \n",
    "    # Create stack\n",
    "    cfn_client.create_stack(\n",
    "        StackName=stack_name,\n",
    "        TemplateBody=template_body,\n",
    "        Capabilities=['CAPABILITY_NAMED_IAM']\n",
    "    )\n",
    "    \n",
    "    print(\"  Waiting for stack creation (5-10 minutes)...\")\n",
    "    cfn_client.get_waiter('stack_create_complete').wait(StackName=stack_name)\n",
    "    \n",
    "    stack = cfn_client.describe_stacks(StackName=stack_name)\n",
    "    print(\"✓ Stack deployed successfully\")\n",
    "\n",
    "# Extract outputs from stack\n",
    "if stack['Stacks']:\n",
    "    outputs = stack['Stacks'][0].get('Outputs', [])\n",
    "    \n",
    "    # Initialize variables\n",
    "    video_analysis_mem_id = None\n",
    "    transcript_mem_id = None\n",
    "    kb_id = None\n",
    "    ds_id = None\n",
    "    data_bucket_name = None\n",
    "    \n",
    "    # Extract all outputs\n",
    "    for output in outputs:\n",
    "        if output['OutputKey'] == 'VideoAnalysisMemoryId':\n",
    "            video_analysis_mem_id = output['OutputValue']\n",
    "        elif output['OutputKey'] == 'LiveTranscriptionMemoryId':\n",
    "            transcript_mem_id = output['OutputValue']\n",
    "        elif output['OutputKey'] == 'ChapterMemoryId':\n",
    "            chapter_mem_id = output['OutputValue']\n",
    "        elif output['OutputKey'] == 'KnowledgeBaseId':\n",
    "            kb_id = output['OutputValue']\n",
    "        elif output['OutputKey'] == 'DataSourceID':\n",
    "            ds_id = output['OutputValue']\n",
    "        elif output['OutputKey'] == 'DataBucketName':\n",
    "            data_bucket_name = output['OutputValue']\n",
    "    \n",
    "    # Session IDs and Actor ID\n",
    "    video_analysis_session_id = \"video-analysis\"\n",
    "    trans_session_id = \"transcripts\"\n",
    "    chapter_session_id = \"chapters\"\n",
    "    actor_id = \"lvu\"\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n✅ Video Analysis Memory: {video_analysis_mem_id}\")\n",
    "    print(f\"✅ Transcript Memory: {transcript_mem_id}\")\n",
    "    print(f\"✅ Chapter Memory ID: {chapter_mem_id}\")\n",
    "    print(f\"✅ Knowledge Base: {kb_id}\")\n",
    "    print(f\"✅ Data Source: {ds_id}\")\n",
    "    print(f\"✅ Data Bucket: {data_bucket_name}\")\n",
    "    \n",
    "    # Store variables for use in other notebooks\n",
    "    %store video_analysis_mem_id\n",
    "    %store video_analysis_session_id\n",
    "    %store transcript_mem_id\n",
    "    %store trans_session_id\n",
    "    %store chapter_mem_id\n",
    "    %store chapter_session_id\n",
    "    %store actor_id\n",
    "    %store kb_id\n",
    "    %store ds_id\n",
    "    %store data_bucket_name\n",
    "    \n",
    "    print(\"\\n✅ Configuration saved!\")\n",
    "else:\n",
    "    print(\"⚠️ Could not retrieve stack outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487917ce-6eb1-47f0-9d53-eb37814388e7",
   "metadata": {},
   "source": [
    "## Download & Process Sample Video Content\n",
    "\n",
    "This section downloads the Netflix Open Content Meridian video file and creates a 2-minute sample clip for use in the workshop notebooks:\n",
    "\n",
    "**Video Content:**\n",
    "- **Full Video**: Netflix Open Content Meridian (complete film)\n",
    "- **Sample Clip**: First 2 minutes for quick testing and demonstrations\n",
    "- **Storage Location**: `sample_videos/` directory in the project root\n",
    "\n",
    "**Files Created:**\n",
    "- `sample_videos/Netflix_Open_Content_Meridian.mp4` - Full video file\n",
    "- `sample_videos/netflix-2mins.mp4` - 2-minute sample clip\n",
    "\n",
    "The sample clip is particularly useful for rapid testing and development without processing the entire film."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a3061-980f-4dae-a14f-39cda3819c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Setting up sample video content...\")\n",
    "\n",
    "# Setup paths\n",
    "sample_videos_dir = Path(\"4-live-media-analysis-agent/sample_videos\")\n",
    "sample_videos_dir.mkdir(exist_ok=True)\n",
    "full_video_path = sample_videos_dir / \"Netflix_Open_Content_Meridian.mp4\"\n",
    "clip_video_path = sample_videos_dir / \"netflix-2mins.mp4\"\n",
    "meridian_url = \"https://ws-assets-prod-iad-r-pdx-f3b3f9f1a7d6a3d0.s3.us-west-2.amazonaws.com/7db2455e-0fa6-4f6d-9973-84daccd6421f/Netflix_Open_Content_Meridian.mp4\"\n",
    "\n",
    "# Download full video if needed\n",
    "if not full_video_path.exists():\n",
    "  print(f\"Downloading {full_video_path.name}...\")\n",
    "  try:\n",
    "      response = requests.get(meridian_url, stream=True)\n",
    "      response.raise_for_status()\n",
    "      with open(full_video_path, 'wb') as f:\n",
    "          for chunk in response.iter_content(chunk_size=8192):\n",
    "              if chunk:\n",
    "                  f.write(chunk)\n",
    "      print(f\"✓ Downloaded ({full_video_path.stat().st_size:,} bytes)\")\n",
    "  except Exception as e:\n",
    "      print(f\"✗ Download failed: {e}\")\n",
    "else:\n",
    "  print(f\"✓ Full video exists ({full_video_path.stat().st_size:,} bytes)\")\n",
    "\n",
    "# Create 2-minute clip if needed\n",
    "if full_video_path.exists() and not clip_video_path.exists():\n",
    "  print(f\"Creating 2-minute clip...\")\n",
    "  try:\n",
    "      cmd = ['ffmpeg', '-i', str(full_video_path), '-t', '120', '-c', 'copy',\n",
    "             '-avoid_negative_ts', 'make_zero', str(clip_video_path)]\n",
    "      result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "      if result.returncode == 0:\n",
    "          print(f\"✓ Clip created ({clip_video_path.stat().st_size:,} bytes)\")\n",
    "      else:\n",
    "          print(f\"✗ Clip creation failed. Install ffmpeg if missing.\")\n",
    "  except FileNotFoundError:\n",
    "      print(\"✗ ffmpeg not found. Install with: brew install ffmpeg (macOS) or apt install ffmpeg (Ubuntu)\")\n",
    "  except Exception as e:\n",
    "      print(f\"✗ Error: {e}\")\n",
    "elif clip_video_path.exists():\n",
    "  print(f\"✓ 2-minute clip exists ({clip_video_path.stat().st_size:,} bytes)\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nStatus: Full video {'✓' if full_video_path.exists() else '✗'} | 2-min clip {'✓' if clip_video_path.exists() else '✗'}\")\n",
    "if full_video_path.exists() and clip_video_path.exists():\n",
    "  print(\"Ready to proceed with video analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd4e668-e7bc-4594-b9e8-d53c9c0834c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Save variables we will use in other notebooks\n",
    "\n",
    "We will use this data in the next labs. In order to use this data we will store these variables so subsequent notebooks can use this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be070591-e890-4147-97ed-c3f77af4eb35",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store sagemaker_resources\n",
    "%store session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacbfe19-95f2-4e6b-8a57-320582d44bff",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Find Amazon Q Developer\n",
    "\n",
    "Jupyter notebooks in SageMaker Studio have Amazon Q Developer enabled.  \n",
    "\n",
    "1. To use Q Developer click on the Q Developer chat icon in the left sidebar menu. The active side panel should now be Amazon Q Developer.\n",
    "<br></br>\n",
    "<img src=\"static/images/00-qdev-sidebar1.png\" alt=\"Q Developer Sidebar\" style=\"width: 600px;\"/>\n",
    "<br></br>\n",
    "5. Try it out by asking a question.  For example, you could ask: `What kinds of questions can Q developer answer? Be brief.` You should get a response like this:\n",
    "<br></br>\n",
    "<img src=\"static/images/00-qdev-skills1.png\" alt=\"Q Developer Skills\" style=\"width: 600px;\"/>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb045c80-93ee-45dd-9c2f-d510e1cc29bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Throughout this workshop, you can use Q when you encounter errors or have questions about the code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3428a57a-e1ef-4f15-8ecc-0ca3a349bf1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
