{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Understanding\n",
    "\n",
    "## üéØ AI-Powered Video Understanding with Adaptive Filmstrip Processing\n",
    "\n",
    "### What is Visual Understanding in Video?\n",
    "\n",
    "Visual understanding means teaching AI to \"see\" and comprehend video content just like humans do. When you watch a video, you naturally understand:\n",
    "- **What's happening** in each video frame\n",
    "- **Who** the characters are and what they're doing\n",
    "- **Where** the action takes place\n",
    "- **How** the story flows from one moment to the next\n",
    "- **Transitions** and why visual transitions matter\n",
    "\n",
    "### Visual Transitions: Understanding Shot Changes\n",
    "\n",
    "Videos are made up of many **shots** - individual camera angles or scenes. When the video cuts from one shot to another (like from a close-up of a person's face to a wide view of a room), this is called a **shot change**. These transitions are a natural part of storytelling and help us understand:\n",
    "- **Shot boundaries** - where one part of the story ends and another begins\n",
    "- **Narrative flow** - how the story is structured and paced\n",
    "- **Important moments** - shot changes often highlight key story points\n",
    "\n",
    "For AI to truly understand video, it needs to recognize these natural transitions just like we do.\n",
    "\n",
    "### Live Stream Visual Understanding\n",
    "\n",
    "Now imagine applying visual understanding to **live streams** - video content that's happening in real-time. This could be:\n",
    "- **Live broadcasts** - news, sports, events happening now\n",
    "- **Video calls** - meetings, interviews, conversations\n",
    "- **Streaming content** - live shows, gaming, tutorials\n",
    "- **Security feeds** - monitoring, surveillance, safety systems\n",
    "\n",
    "Live stream visual understanding means AI can analyze and comprehend video content **as it happens**, providing real-time insights about what's being seen. This opens up powerful possibilities like automatic content moderation, identifying key moments, instant highlight detection, and live content summarization.\n",
    "\n",
    "### The Multi-Dimensional Challenge\n",
    "\n",
    "But live stream visual understanding faces challenges across multiple dimensions:\n",
    "\n",
    "**Data & Performance Challenges:**\n",
    "- **Too Much Data**: Videos may contain huge number of frames per minute\n",
    "- **Real-time Speed**: We need fast processing without delays\n",
    "- **Storage Limits**: Can't store every single frame or handle high resolution content (such as 4K)\n",
    "- **Quality vs Quantity**: Need to balance detail with coverage\n",
    "\n",
    "**Model Constraints:**\n",
    "\n",
    "AI models like Claude have strict limits:\n",
    "- **File Size**: Maximum 3.75MB per image\n",
    "- **Dimensions**: Maximum 8000√ó8000 pixels\n",
    "- **Image Count**: Limited number of images per analysis (20)\n",
    "\n",
    "### Our Solution: Adaptive Filmstrip Processing\n",
    "\n",
    "We address these challenges with a smart technique:\n",
    "\n",
    "- **üé¨ Film Grid**: Pack multiple video frames into organized grid layouts (like a photo collage)\n",
    "- **üìä Multiple Film Grids**: Create several grid images to cover the entire video\n",
    "- **üîç Shot Change Detection**: Automatically identify natural shot transitions\n",
    "- **‚ö° Adaptive Processing**: Automatically optimize everything to fit AI limits based on the source charateristics to maintain accuracy. \n",
    "\n",
    "**Let's see how this works in practice!** üé¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Parameters\n",
    "\n",
    "**Set all processing parameters** - These settings control how the video gets prepared for AI analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video Configuration\n",
    "VIDEO_FILE = '../sample_videos/netflix-2mins.mp4'\n",
    "OUTPUT_PREFIX = 'output/filmstrip'\n",
    "OUTPUT_DIR = 'output'\n",
    "\n",
    "# Grid Configuration\n",
    "MAX_GRID_WIDTH = 8000\n",
    "MAX_GRID_HEIGHT = 8000\n",
    "MAX_GRID_IMAGES = 20\n",
    "FIXED_GRID_ROWS = 4\n",
    "FIXED_GRID_COLS = 5\n",
    "PRESERVE_SOURCE_RESOLUTION = True\n",
    "MAX_FILE_SIZE_MB = 2.0  # Maximum file size per grid (None = no limit)\n",
    "\n",
    "# Duration Control\n",
    "START_TIME = 0.0\n",
    "PROCESS_DURATION = None  # None = Process entire video\n",
    "\n",
    "# Visual Configuration\n",
    "BORDER_THICKNESS = 8\n",
    "LABEL_HEIGHT = 40\n",
    "BORDER_COLOR = 'red'\n",
    "LABEL_BG_COLOR = 'black'\n",
    "LABEL_TEXT_COLOR = 'white'\n",
    "\n",
    "# Shot Change Detection\n",
    "ENABLE_SHOT_DETECTION = True\n",
    "SHOT_DETECTION_THRESHOLD = 0.3  # Lower = more sensitive (0.0-1.0)\n",
    "\n",
    "# Bedrock Configuration\n",
    "CLAUDE_MODEL_ID = 'global.anthropic.claude-sonnet-4-20250514-v1:0'\n",
    "MAX_GRIDS_TO_ANALYZE = 5\n",
    "AWS_REGION=\"us-east-1\"\n",
    "print('‚úÖ Configuration loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "**Load required modules** for video processing, AI analysis, and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "import cv2\n",
    "from IPython.display import Video, display\n",
    "\n",
    "sys.path.insert(0, '../src')\n",
    "from shared.filmstrip_processor import AdaptiveFilmstripProcessor\n",
    "from shared.shot_change_detector import create_fusion_detector\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import boto3\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print('‚úÖ Imports successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preview Source Video\n",
    "\n",
    "**Check the video** before processing. This shows you basic video information and lets you watch it.\n",
    "\n",
    "### Source Video\n",
    "\n",
    "For this workshop, we will be using **Meridian, 2016**, Mystery from [Netflix](https://opencontent.netflix.com/#h.fzfk5hndrb9w). This video is available under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if video file exists\n",
    "if os.path.exists(VIDEO_FILE):\n",
    "    # Get video properties\n",
    "    cap = cv2.VideoCapture(VIDEO_FILE)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    duration = frame_count / fps if fps > 0 else 0\n",
    "    cap.release()\n",
    "    \n",
    "    print('üìπ Video Information:')\n",
    "    print(f'   File: {VIDEO_FILE}')\n",
    "    print(f'   Duration: {duration:.1f}s ({duration/60:.1f} minutes)')\n",
    "    print(f'   Resolution: {width}√ó{height}px')\n",
    "    print(f'   FPS: {fps:.1f}')\n",
    "    print(f'   Total Frames: {frame_count:,}')\n",
    "    print(f'\\n   Processing: {START_TIME:.1f}s to {START_TIME + (PROCESS_DURATION or duration):.1f}s')\n",
    "    \n",
    "    # Display video player\n",
    "    print('\\nüé¨ Video Player:')\n",
    "    display(Video(VIDEO_FILE, width=800, height=450))\n",
    "else:\n",
    "    print(f'‚ùå Video file not found: {VIDEO_FILE}')\n",
    "    print('   Please check the VIDEO_FILE path in the configuration cell.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What We're Looking At\n",
    "\n",
    "Before we start, let's understand some of the key video information:\n",
    "- The **duration** tells us how long the video is - longer videos will take more time to process.\n",
    "- The **resolution** shows the video quality (like 1080p or 720p) - higher quality videos create larger files and may take longer to process. \n",
    "- **FPS (frames per second)** tells us how smooth the video is - most videos are 24-30 FPS. \n",
    "- The **total frames** shows how much data we have to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Adaptive Filmstrip Processor\n",
    "\n",
    "**Set up the adaptive processor** that automatically prepares optimal filmstrip grid images from the video for AI analysis.\n",
    "\n",
    "### üéØ How This Works\n",
    "\n",
    "Think of this like creating a photo collage from a long video. Here's what happens:\n",
    "\n",
    "**Sample Video:**\n",
    "- 2 minutes long (120 seconds)\n",
    "- HD quality (1280√ó720)\n",
    "- 30 frames per second\n",
    "- Total: 3,600 individual frames\n",
    "\n",
    "**The Challenge:**\n",
    "- AI can only handle images up to 3.75MB\n",
    "- Images can't be bigger than 8000√ó8000 pixels\n",
    "- We want to analyze as much video as possible\n",
    "\n",
    "**Adaptive Filmstrip Process:**\n",
    "\n",
    "1. **Grid Layout Determination**\n",
    "   - Use provided grid matrix if specified by user (we will use 4√ó5 user-defined matrix for this workshop)\n",
    "   - Otherwise, calculate optimal matrix: AI limit (8000√ó8000) √∑ source resolution (1280√ó720) = max 6√ó11 grid\n",
    "   - Example: Use 4√ó5 grid matrix (20 frames per image)\n",
    "\n",
    "2. **Frame Packaging Capacity**\n",
    "   - AI image limit: 20 images maximum\n",
    "   - Grid capacity: 4√ó5 = 20 frames per image\n",
    "   - Total packageable frames: 20 images √ó 20 frames = 400 frames\n",
    "\n",
    "3. **Size Optimization**\n",
    "   - Downscale overall grid image to fit 2MB limit using compression formulas\n",
    "   - Balance quality with file size constraints\n",
    "\n",
    "4. **Frame Sampling Strategy**\n",
    "   - Source video: 120 seconds √ó 25 FPS = 3,600 total frames\n",
    "   - Sampling interval: 3,600 frames √∑ 400 packageable frames = 9\n",
    "   - Extract every 9th frame for comprehensive coverage\n",
    "\n",
    "**The Result:**\n",
    "- 20 frames placed in 4√ó5 matrix in single grid image (20 frames in one image)\n",
    "- 20 grid images created covering 400 frames in total\n",
    "- Each image: ~2MB (fits AI limits perfectly)\n",
    "- Sample interval: 9 (pick every 9th frame)\n",
    "- Timestamp footer added in each cell\n",
    "- Scene changes automatically detected\n",
    "\n",
    "### üí° Why This Matters\n",
    "\n",
    "Instead of manually figuring out sizes, sampling rates, and formats, the processor does all the math automatically. You just set your preferences, and it handles the complex process of creating filmstrip grid images from the given video!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create shot change detector if enabled\n",
    "shot_detector = None\n",
    "if ENABLE_SHOT_DETECTION:\n",
    "    shot_detector = create_fusion_detector(threshold=SHOT_DETECTION_THRESHOLD)\n",
    "    print(f'‚úÖ Shot detector created (threshold={SHOT_DETECTION_THRESHOLD})')\n",
    "\n",
    "processor = AdaptiveFilmstripProcessor(\n",
    "    max_grid_size=(MAX_GRID_WIDTH, MAX_GRID_HEIGHT),\n",
    "    max_grid_images=MAX_GRID_IMAGES,\n",
    "    fixed_grid_layout=(FIXED_GRID_ROWS, FIXED_GRID_COLS),\n",
    "    preserve_source_resolution=PRESERVE_SOURCE_RESOLUTION,\n",
    "    max_file_size_mb=MAX_FILE_SIZE_MB,\n",
    "    border_thickness=BORDER_THICKNESS,\n",
    "    label_height=LABEL_HEIGHT,\n",
    "    border_color=BORDER_COLOR,\n",
    "    label_bg_color=LABEL_BG_COLOR,\n",
    "    label_text_color=LABEL_TEXT_COLOR,\n",
    "    shot_detector=shot_detector\n",
    ")\n",
    "\n",
    "print('‚úÖ Processor created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process Video with the Adaptive Filmstrip Processor\n",
    "\n",
    "**Run the adaptive processor** - This is where the video gets transformed into grid images for AI analysis!\n",
    "\n",
    "### What Happens Now\n",
    "\n",
    "When you run this step, the system automatically:\n",
    "\n",
    "1. **Analyzes the video** - Looks at duration, resolution, and frame rate\n",
    "2. **Caclulate the optimal packaging** - Identifies the grid layout matrix, number of images, and frame sampling based on the video information and AI limits.\n",
    "3. **Picks the frames** - Selects representative frames throughout the video\n",
    "4. **Adds frame timestamps** - Labels each frame with its exact timestamp in the video\n",
    "5. **Creates grid images** - Packs multiple frames into organized collages\n",
    "6. **Detects shot changes** - Finds where the video transitions between frames \n",
    "\n",
    "üí° **The Result**: Multiple grid images ready for AI analysis, each containing 20 video frames with timestamps, along with the shot changes information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = processor.create_adaptive_filmstrips(\n",
    "    video_file=VIDEO_FILE,\n",
    "    output_prefix=OUTPUT_PREFIX,\n",
    "    start_time=START_TIME,\n",
    "    process_duration=PROCESS_DURATION,\n",
    "    detect_shot_changes=ENABLE_SHOT_DETECTION\n",
    ")\n",
    "\n",
    "layout = result['layout']\n",
    "output_files = result['output_files']\n",
    "\n",
    "print(f'\\n‚úÖ Created {len(output_files)} filmstrip grids')\n",
    "print(f'   Frames per grid: {layout[\"frames_per_grid\"]}')\n",
    "print(f'   Total frames: {layout[\"frames_to_extract\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Review Adaptive Filmstrip Processor Output\n",
    "\n",
    "**Check what the processor has created** - Let's see how the video was prepared for AI analysis.\n",
    "\n",
    "### Understanding the Results\n",
    "\n",
    "The results show you exactly what happened to the video:\n",
    "\n",
    "- **Grid Layout**: How frames are arranged (like 4√ó5 = 20 frames per image)\n",
    "- **Frame Size**: The dimensions of each individual frame in the grid\n",
    "- **Frames Extracted**: How many frames were selected vs. total available\n",
    "- **Sampling Rate**: The pattern used (like \"every 10th frame\")\n",
    "- **Coverage**: What percentage of the video is included\n",
    "- **Shot Changes**: How many frame transitions were found\n",
    "\n",
    "üí° **Why This Matters**: These numbers help you understand the balance between video coverage and AI limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLayout Summary:')\n",
    "print(f'  Grid: {layout[\"grid_rows\"]}√ó{layout[\"grid_cols\"]}')\n",
    "print(f'  Frame size: {layout[\"cell_size\"][0]}√ó{layout[\"cell_size\"][1]}px')\n",
    "print(f'  Frames extracted: {layout[\"frames_to_extract\"]}/{layout[\"total_frames\"]}')\n",
    "sampling_text = f'Every {layout[\"sampling_rate\"]} frames' if layout[\"sampling_rate\"] > 1 else 'No sampling'\n",
    "print(f'  Sampling: {sampling_text}')\n",
    "print(f'  Coverage: {(layout[\"frames_to_extract\"]/layout[\"total_frames\"])*100:.1f}%')\n",
    "\n",
    "# Display shot change summary\n",
    "if ENABLE_SHOT_DETECTION:\n",
    "    total_shots = sum(len(sc['shot_changes']) for sc in result['shot_changes'])\n",
    "    print(f'\\n  Shot changes detected: {total_shots}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Filmstrip Grids\n",
    "\n",
    "**View filmstrip grids** - See how the entire video is intelligently packed into multiple grid images for AI analysis.\n",
    "\n",
    "### Multiple Grid Images Created\n",
    "\n",
    "The system creates **multiple grid images** (not just one), each containing:\n",
    "\n",
    "- **20 frames per grid** arranged in 4√ó5 layout\n",
    "- **Smart packing** to fit AI model limits (under 2MB each)\n",
    "- **Sequential coverage** - Grid 1 ‚Üí Grid 2 ‚Üí Grid 3 covers the entire video\n",
    "- **Timestamps and position labels** for precise frame referencing\n",
    "- **Optimized file sizes** that balance quality with AI constraints\n",
    "\n",
    "### Why Multiple Grids Work Well\n",
    "\n",
    "Instead of trying to fit everything in one oversized image, we create multiple optimized grids:\n",
    "- **Comprehensive coverage** of the entire video\n",
    "- **AI model compatibility** - each grid fits within size and dimension limits\n",
    "- **Efficient processing** - 20√ó more efficient than individual frames\n",
    "- **Perfect for referencing** specific moments across the full video\n",
    "\n",
    "üí° **Smart approach**: Multiple grids = complete video coverage within AI limits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display filmstrip grids with red box highlighting the focused frame\n",
    "for i, file_path in enumerate(output_files[:1]):\n",
    "    if os.path.exists(file_path):\n",
    "        print(f'\\nGrid {i+1}: {file_path}')\n",
    "        img = mpimg.imread(file_path)\n",
    "        \n",
    "        # Calculate frame dimensions for red box overlay\n",
    "        img_height, img_width = img.shape[:2]\n",
    "        frame_width = (img_width - (FIXED_GRID_COLS + 1) * BORDER_THICKNESS) // FIXED_GRID_COLS\n",
    "        frame_height_with_label = (img_height - (FIXED_GRID_ROWS + 1) * BORDER_THICKNESS) // FIXED_GRID_ROWS\n",
    "        \n",
    "        # Select middle frame (row 2, col 3 for a 4x5 grid)\n",
    "        focus_row, focus_col = 1, 2  # 0-indexed (displays as [2√ó3])\n",
    "        \n",
    "        # Calculate frame position for red box\n",
    "        start_x = BORDER_THICKNESS + focus_col * (frame_width + BORDER_THICKNESS)\n",
    "        start_y = BORDER_THICKNESS + focus_row * (frame_height_with_label + BORDER_THICKNESS)\n",
    "        end_x = start_x + frame_width\n",
    "        end_y = start_y + frame_height_with_label\n",
    "        \n",
    "        # Display full grid with red box highlight\n",
    "        fig, ax = plt.subplots(figsize=(16, 12))\n",
    "        ax.imshow(img)\n",
    "        \n",
    "        # Add yellow box around the focused frame\n",
    "        from matplotlib.patches import Rectangle\n",
    "        yellow_box = Rectangle((start_x-5, start_y-5), frame_width+10, frame_height_with_label+10, \n",
    "                           linewidth=4, edgecolor='yellow', facecolor='none', linestyle='-')\n",
    "        ax.add_patch(yellow_box)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "if len(output_files) > 1:\n",
    "    print(f'\\n... and {len(output_files) - 1} more grids')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What We're Seeing\n",
    "\n",
    "Each grid image is like a photo collage containing multiple video frames. This section shows you:\n",
    "\n",
    "- **Individual Frames**: Specific moments from the video\n",
    "- **Timestamps**: Exactly when each frame occurs in the video\n",
    "- **Position Labels**: Where each frame sits in the grid (like [2√ó3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Examine Individual Frame Within A Grid Image\n",
    "\n",
    "**Look at the individual frames up close** - Let's closely examine the frame within yellow box from one of the grid images shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display focused frames with detailed analysis\n",
    "print('Focused Frame Analysis - Detailed View of Individual Frames\\n')\n",
    "\n",
    "for i, file_path in enumerate(output_files[:1]):\n",
    "    if os.path.exists(file_path):\n",
    "        img = mpimg.imread(file_path)\n",
    "        \n",
    "        # Calculate frame dimensions (accounting for borders and labels)\n",
    "        img_height, img_width = img.shape[:2]\n",
    "        frame_width = (img_width - (FIXED_GRID_COLS + 1) * BORDER_THICKNESS) // FIXED_GRID_COLS\n",
    "        frame_height_with_label = (img_height - (FIXED_GRID_ROWS + 1) * BORDER_THICKNESS) // FIXED_GRID_ROWS\n",
    "        frame_height = frame_height_with_label - LABEL_HEIGHT\n",
    "        \n",
    "        # Select middle frame (row 2, col 3 for a 4x5 grid)\n",
    "        focus_row, focus_col = 1, 2  # 0-indexed (displays as [2√ó3])\n",
    "        \n",
    "        # Calculate frame position in the image\n",
    "        start_x = BORDER_THICKNESS + focus_col * (frame_width + BORDER_THICKNESS)\n",
    "        start_y = BORDER_THICKNESS + focus_row * (frame_height_with_label + BORDER_THICKNESS)\n",
    "        end_x = start_x + frame_width\n",
    "        end_y = start_y + frame_height_with_label\n",
    "        \n",
    "        # Extract the frame (including footer)\n",
    "        frame_img = img[start_y:end_y, start_x:end_x]\n",
    "        \n",
    "        # Calculate timestamp for this frame (matching the processor's logic)\n",
    "        frame_index = focus_row * FIXED_GRID_COLS + focus_col\n",
    "        frames_per_grid = layout['frames_per_grid']\n",
    "        global_frame_index = i * frames_per_grid + frame_index\n",
    "        # Match the processor's timestamp calculation: time_offset = i * interval + (interval / 2)\n",
    "        time_offset = global_frame_index * layout['extraction_interval'] + (layout['extraction_interval'] / 2)\n",
    "        timestamp = START_TIME + time_offset\n",
    "        \n",
    "        print(f'Grid {i+1} - Focused Frame Analysis:')\n",
    "        print(f'   Position: [{focus_row+1}√ó{focus_col+1}] (Row {focus_row+1}, Column {focus_col+1})')\n",
    "        print(f'   Timestamp: {timestamp:.1f}s')\n",
    "        print(f'   Global Frame Index: {global_frame_index}')\n",
    "        print(f'   Frame Dimensions: {frame_width}√ó{frame_height_with_label}px')\n",
    "        \n",
    "        # Display the focused frame in large size\n",
    "        fig, ax = plt.subplots(figsize=(20, 15))\n",
    "        ax.imshow(frame_img)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Add title with position and timestamp info\n",
    "        title = f'Focused Frame [{focus_row+1}√ó{focus_col+1}] | {timestamp:.1f}s | Grid {i+1}'\n",
    "        ax.set_title(title, fontsize=20, fontweight='bold', pad=30)\n",
    "        \n",
    "        # Add detailed footer information with red background for emphasis\n",
    "        footer_text = f'Position: Row {focus_row+1}, Column {focus_col+1} | Timestamp: {timestamp:.1f}s | Frame: {global_frame_index} | Size: {frame_width}√ó{frame_height_with_label}px'\n",
    "        plt.figtext(0.5, 0.02, footer_text, ha='center', fontsize=14, fontweight='bold',\n",
    "                   bbox=dict(boxstyle='round,pad=0.8', facecolor='red', alpha=0.8, edgecolor='darkred'))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(bottom=0.12)  # Make room for footer\n",
    "        plt.show()\n",
    "        print('\\n' + '='*80 + '\\n')\n",
    "\n",
    "print('Focused frame analysis complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why This Helps\n",
    "\n",
    "When AI analyzes the video, it can reference specific moments by their grid position and timestamp. This makes the analysis much more precise and useful.\n",
    "\n",
    "üí° **Example**: AI might say \"At position [2√ó3] around 15.5 seconds, the character enters the room\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Shot Change Detection Results\n",
    "\n",
    "**See where scenes change in the video** - The system automatically finds visual transitions between different frames.\n",
    "\n",
    "### How This Works:\n",
    "1. **Compare Frames**: Look at consecutive video frames\n",
    "2. **Measure Differences**: Check how much the image changes\n",
    "3. **Find Big Changes**: When the change is significant, it's probably a new shot\n",
    "4. **Record the Moment**: Save the timestamp and location\n",
    "\n",
    "### Why This Matters:\n",
    "- Helps AI understand the story flow\n",
    "- Makes analysis more accurate\n",
    "- Provides context about video structure\n",
    "\n",
    "üí° **Simple**: The system automatically finds shot changes in the video by comparing and measuring differences between frame images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shot changes are now included in the result with timestamps!\n",
    "shot_changes_info = result['shot_changes']  # Already has timestamps and positions\n",
    "\n",
    "if ENABLE_SHOT_DETECTION:\n",
    "    print('\\nShot Change Details:')\n",
    "    print('=' * 80)\n",
    "    \n",
    "    # Check if we have the new format with shot_segments\n",
    "    has_shot_segments = shot_changes_info and 'shot_segments' in shot_changes_info[0]\n",
    "    \n",
    "    if has_shot_segments:\n",
    "        # New format: shot_segments already calculated by processor\n",
    "        for grid_data in shot_changes_info:\n",
    "            grid_idx = grid_data['grid_index']\n",
    "            shot_segments = grid_data['shot_segments']\n",
    "            \n",
    "            if shot_segments:\n",
    "                print(f'\\nGrid {grid_idx + 1}:')\n",
    "                for shot in shot_segments:\n",
    "                    print(f'  ‚Ä¢ Shot change at [{shot[\"row\"]}√ó{shot[\"col\"]}] | {shot[\"timestamp\"]:.1f}s')\n",
    "        \n",
    "        print('\\n' + '=' * 80)\n",
    "        total_shots = sum(len(sc['shot_segments']) for sc in shot_changes_info)\n",
    "        print(f'Total shot changes: {total_shots}')\n",
    "        print('\\nüí° Timestamps and grid positions calculated by AdaptiveFilmstripProcessor')\n",
    "    else:\n",
    "        # Old format: need to calculate timestamps (backward compatibility)\n",
    "        print('\\n‚ö†Ô∏è  Using old result format. Re-run processing cell for enhanced shot data.')\n",
    "        print('\\nCalculating timestamps from old format...')\n",
    "        \n",
    "        for grid_data in shot_changes_info:\n",
    "            grid_idx = grid_data['grid_index']\n",
    "            shot_changes = grid_data['shot_changes']\n",
    "            frame_range = grid_data['frame_range']\n",
    "            \n",
    "            if shot_changes:\n",
    "                print(f'\\nGrid {grid_idx + 1}:')\n",
    "                for shot_idx in shot_changes:\n",
    "                    row = (shot_idx // FIXED_GRID_COLS) + 1\n",
    "                    col = (shot_idx % FIXED_GRID_COLS) + 1\n",
    "                    global_frame_idx = frame_range[0] + shot_idx\n",
    "                    # Match the processor's timestamp calculation\n",
    "                    time_offset = global_frame_idx * layout['extraction_interval'] + (layout['extraction_interval'] / 2)\n",
    "                    timestamp = START_TIME + time_offset\n",
    "                    print(f'  ‚Ä¢ Shot change at [{row}√ó{col}] | {timestamp:.1f}s')\n",
    "        \n",
    "        print('\\n' + '=' * 80)\n",
    "        total_shots = sum(len(sc['shot_changes']) for sc in shot_changes_info)\n",
    "        print(f'Total shot changes: {total_shots}')\n",
    "else:\n",
    "    print('\\nShot change detection disabled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Shot Change Detection\n",
    "\n",
    "**See scene changes in action** - Compare what the video looked like before and after a scene change.\n",
    "\n",
    "### What You'll See:\n",
    "- **BEFORE Frame**: What the video looked like in the previous scene\n",
    "- **AFTER Frame**: What the video looks like in the new scene\n",
    "- **The Difference**: How much the visual content changed\n",
    "\n",
    "### Understanding the Numbers:\n",
    "- **Similarity Score**: How similar the frames are (0 = totally different, 1 = identical)\n",
    "- **Threshold**: The cutoff point for detecting changes (we use 0.3)\n",
    "- **Detection**: If similarity is below 0.3, we found a scene change!\n",
    "\n",
    "üí° **Simple Rule**: Big visual changes = new scenes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if ENABLE_SHOT_DETECTION and shot_changes_info:\n",
    "    # Find first shot change to visualize\n",
    "    first_shot = None\n",
    "    for grid_info in shot_changes_info:\n",
    "        if grid_info['shot_segments']:\n",
    "            first_shot = grid_info['shot_segments'][0]\n",
    "            grid_idx = grid_info['grid_index']\n",
    "            break\n",
    "    \n",
    "    if first_shot:\n",
    "        print('\\nüé¨ Visualizing Shot Change Detection')\n",
    "        print(f'   Location: Grid {grid_idx + 1}, Frame [{first_shot[\"row\"]}√ó{first_shot[\"col\"]}]')\n",
    "        print(f'   Timestamp: {first_shot[\"timestamp\"]:.1f}s')\n",
    "        print(f'   Frame Index: {first_shot[\"frame_index\"]}')\n",
    "        \n",
    "        # Open video and extract frames\n",
    "        cap = cv2.VideoCapture(VIDEO_FILE)\n",
    "        \n",
    "        # Calculate frame positions\n",
    "        frame_idx = first_shot['frame_index']\n",
    "        # Match the processor's timestamp calculation\n",
    "        before_time_offset = (frame_idx - 1) * layout['extraction_interval'] + (layout['extraction_interval'] / 2)\n",
    "        after_time_offset = frame_idx * layout['extraction_interval'] + (layout['extraction_interval'] / 2)\n",
    "        before_time = START_TIME + before_time_offset\n",
    "        after_time = START_TIME + after_time_offset\n",
    "        \n",
    "        # Extract before frame (frame before shot change)\n",
    "        cap.set(cv2.CAP_PROP_POS_MSEC, before_time * 1000)\n",
    "        ret1, frame_before = cap.read()\n",
    "        \n",
    "        # Extract after frame (frame where shot change detected)\n",
    "        cap.set(cv2.CAP_PROP_POS_MSEC, after_time * 1000)\n",
    "        ret2, frame_after = cap.read()\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        if ret1 and ret2:\n",
    "            # Convert BGR to RGB for display\n",
    "            frame_before_rgb = cv2.cvtColor(frame_before, cv2.COLOR_BGR2RGB)\n",
    "            frame_after_rgb = cv2.cvtColor(frame_after, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Create side-by-side visualization\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "            \n",
    "            # Before frame\n",
    "            axes[0].imshow(frame_before_rgb)\n",
    "            axes[0].set_title(f'BEFORE Shot Change\\nFrame {frame_idx - 1} | {before_time:.1f}s', \n",
    "                            fontsize=14, fontweight='bold', color='blue')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            # After frame\n",
    "            axes[1].imshow(frame_after_rgb)\n",
    "            axes[1].set_title(f'AFTER Shot Change (Detected)\\nFrame {frame_idx} | {after_time:.1f}s', \n",
    "                            fontsize=14, fontweight='bold', color='red')\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            plt.suptitle('Shot Change Detection: Before vs After', \n",
    "                        fontsize=16, fontweight='bold', y=0.98)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Calculate and display histogram difference\n",
    "            hsv_before = cv2.cvtColor(frame_before, cv2.COLOR_BGR2HSV)\n",
    "            hsv_after = cv2.cvtColor(frame_after, cv2.COLOR_BGR2HSV)\n",
    "            \n",
    "            hist_before = cv2.calcHist([hsv_before], [0, 1, 2], None, [8, 8, 8], [0, 180, 0, 256, 0, 256])\n",
    "            hist_after = cv2.calcHist([hsv_after], [0, 1, 2], None, [8, 8, 8], [0, 180, 0, 256, 0, 256])\n",
    "            \n",
    "            correlation = cv2.compareHist(hist_before, hist_after, cv2.HISTCMP_CORREL)\n",
    "            \n",
    "            print('\\n   üìä Detection Metrics:')\n",
    "            print(f'      Histogram Correlation: {correlation:.4f}')\n",
    "            print(f'      Threshold: {SHOT_DETECTION_THRESHOLD}')\n",
    "            print(f'      Shot Detected: {\"YES\" if correlation < SHOT_DETECTION_THRESHOLD else \"NO\"} '\n",
    "                  f'({correlation:.4f} < {SHOT_DETECTION_THRESHOLD})')\n",
    "            print('\\n   üí° Lower correlation = more different frames = shot change')\n",
    "        else:\n",
    "            print('   ‚ùå Could not extract frames for visualization')\n",
    "    else:\n",
    "        print('\\n   No shot changes detected to visualize')\n",
    "else:\n",
    "    print('\\n   Shot change detection disabled or no changes detected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Define Helper Functions\n",
    "\n",
    "**Create utilities for Claude interaction** - These functions prepare data and prompts for AI analysis.\n",
    "\n",
    "### What These Functions Do\n",
    "\n",
    "Think of these as the AI communication toolkit:\n",
    "\n",
    "1. **Image Encoder**: Converts the grid images into a format AI can read\n",
    "2. **Prompt Builder**: Creates clear instructions for AI about how to analyze the video\n",
    "3. **API Communicator**: Sends everything to Claude and gets the analysis back\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "AI needs specific formats and clear instructions to do its best work. These functions handle all the technical details so you don't have to worry about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_to_base64(image_path):\n",
    "    with open(image_path, 'rb') as f:\n",
    "        return base64.b64encode(f.read()).decode('utf-8')\n",
    "\n",
    "def create_analysis_prompt(grid_rows, grid_cols, num_grids, shot_changes_info=None):\n",
    "    prompt = f\"\"\"You are analyzing filmstrip grids for comprehensive VISUAL UNDERSTANDING of video content.\n",
    "\n",
    "    GRID SPECIFICATIONS:\n",
    "    ‚Ä¢ Grid Structure: {grid_rows}√ó{grid_cols} = {grid_rows * grid_cols} frames per grid\n",
    "    ‚Ä¢ Total Grids: {num_grids}\n",
    "    ‚Ä¢ Reading Order: LEFT‚ÜíRIGHT, TOP‚ÜíBOTTOM within each grid, then Grid 1‚Üí2‚Üí3 sequentially\n",
    "    ‚Ä¢ Frame Labels: [row√ócol] | timestamp format\n",
    "    \n",
    "    VISUAL ANALYSIS FRAMEWORK:\n",
    "    Examine each frame for objects, people, environments, visual elements, and text. Track how these elements change across time.\"\"\"\n",
    "\n",
    "    # Add shot change information if available\n",
    "    if shot_changes_info:\n",
    "        prompt += \"\\n\\nüéØ DETECTED SCENE TRANSITIONS:\\n\"\n",
    "        for grid_info in shot_changes_info:\n",
    "            grid_idx = grid_info['grid_index']\n",
    "            shots = grid_info['shot_segments']\n",
    "            if shots:\n",
    "                prompt += f\"\\nGrid {grid_idx + 1} - Visual Transitions:\\n\"\n",
    "                for shot in shots:\n",
    "                    prompt += f\"  ‚ñ∂ Visual change at {shot['timestamp']:.1f}s [Frame {shot['row']}√ó{shot['col']}] - New visual scene\\n\"\n",
    "        prompt += \"\\nüí° Use these transitions to identify major visual shifts and scene boundaries.\\n\"\n",
    "    \n",
    "    prompt += \"\"\"\n",
    "    REQUIRED VISUAL ANALYSIS:\n",
    "    \n",
    "    1. **TEXT RECOGNITION & ANALYSIS**\n",
    "       - Signs, billboards, street names, building labels\n",
    "       - On-screen text, titles, captions, subtitles\n",
    "       - License plates, product names, brand logos\n",
    "       - Written content in documents, books, newspapers\n",
    "       - Digital displays, screens, monitors showing text\n",
    "       - Handwritten text or notes visible in frames\n",
    "    \n",
    "    2. **OBJECT DETECTION & IDENTIFICATION**\n",
    "       - People: Count, positions, actions, clothing, expressions\n",
    "       - Vehicles: Types, colors, positions, movement\n",
    "       - Buildings/Architecture: Structures, styles, conditions\n",
    "       - Natural elements: Trees, sky, weather, terrain\n",
    "       - Props/Items: Tools, furniture, signs, technology\n",
    "    \n",
    "    3. **SPATIAL COMPOSITION**\n",
    "       - Foreground, middle ground, background elements\n",
    "       - Object placement and relationships\n",
    "       - Scale and perspective of objects\n",
    "       - Depth and layering in scenes\n",
    "    \n",
    "    4. **VISUAL ENVIRONMENT**\n",
    "       - Indoor vs outdoor settings\n",
    "       - Lighting conditions (natural, artificial, time of day)\n",
    "       - Weather and atmospheric conditions\n",
    "       - Geographic or architectural context\n",
    "    \n",
    "    5. **MOVEMENT & DYNAMICS**\n",
    "       - Object motion patterns across frames\n",
    "       - People walking, vehicles moving, environmental changes\n",
    "       - Camera movement effects on object positions\n",
    "       - Temporal changes in object states\n",
    "    \n",
    "    6. **COLOR & VISUAL PROPERTIES**\n",
    "       - Dominant color schemes per scene\n",
    "       - Object colors and visual characteristics\n",
    "       - Lighting effects on appearance\n",
    "       - Visual quality and clarity changes\n",
    "    \n",
    "    7. **SCENE CONTEXT & SETTING**\n",
    "       - Location types (street, building, park, etc.)\n",
    "       - Time period indicators from visual cues\n",
    "       - Cultural or regional visual markers\n",
    "       - Activity contexts from visible elements\n",
    "    \n",
    "    8. **CONTENT MODERATION ANALYSIS**\n",
    "       - Violence: Weapons, fighting, aggressive behavior, injuries\n",
    "       - Adult content: Nudity, suggestive poses, intimate situations\n",
    "       - Substance use: Smoking, drinking, drug paraphernalia\n",
    "       - Disturbing content: Blood, graphic imagery, distressing scenes\n",
    "       - Inappropriate behavior: Dangerous activities, harmful actions\n",
    "       - Age-inappropriate elements: Content unsuitable for minors\n",
    "    \n",
    "    9. **NARRATIVE INFERENCE**\n",
    "       - Story context derived from visual and textual cues\n",
    "       - Character relationships and interactions\n",
    "       - Plot progression indicated by visual elements\n",
    "       - Setting and time period from environmental clues\n",
    "       - Emotional tone and atmosphere\n",
    "       - Thematic elements suggested by visuals and text\n",
    "    \n",
    "    For each element (visual, textual, or narrative), reference specific frames using [row√ócol] notation and timestamps. \n",
    "    Combine text recognition with visual analysis to provide comprehensive understanding of the video's content and story.\n",
    "\n",
    "    OUTPUT FORMAT REQUIREMENT:\n",
    "    You MUST respond with ONLY valid JSON in the following structure:\n",
    "    \n",
    "    {\n",
    "      \"video_analysis\": {\n",
    "        \"overview\": {\n",
    "          \"title\": \"Brief title of the video content\",\n",
    "          \"duration_analyzed\": \"Duration in seconds\",\n",
    "          \"total_frames_analyzed\": \"Number of frames analyzed\",\n",
    "          \"genre\": \"Video genre/type\",\n",
    "          \"summary\": \"Comprehensive summary of the entire video content\"\n",
    "        },\n",
    "        \"text_recognition\": {\n",
    "          \"details\" [ details of text extracted\" ]\n",
    "        },\n",
    "        \"movement_dynamics\": {\n",
    "            \"details\" : [ \"Details of the movement and dynamics\" ]\n",
    "        },\n",
    "        \"spatial_compositions\": {\n",
    "            \"details\" : [ \"spatial compositions details \" ]\n",
    "        },\n",
    "        \"color_visual_properties\": {\n",
    "            \"details\": [ \"color and visual details\" ]\n",
    "        },\n",
    "        \"visual_elements\": {\n",
    "            \"people_details\": [ \"Details of the poeple identified\" ],\n",
    "            \"object_details\": [ \"Object Details\" ],\n",
    "            \"environment_details\": [ \"Details of the environment\"]\n",
    "\n",
    "        },\n",
    "        \"content_moderation\": {\n",
    "          \"details\": [ \"content moderation details\" ]\n",
    "        },\n",
    "        \"narrative_analysis\": {\n",
    "          \"details\": [ Narrative analysis details\" ]\n",
    "        },\n",
    "        \"chapters\": [\n",
    "          {\n",
    "            \"chapter_number\": 1,\n",
    "            \"title\": \"Descriptive chapter title\",\n",
    "            \"start_time\": \"Start timestamp in seconds\",\n",
    "            \"end_time\": \"End timestamp in seconds\",\n",
    "            \"duration\": \"Chapter duration in seconds\",\n",
    "            \"description\": \"Detailed description of what happens in this chapter\",\n",
    "            \"key_events\": [\"List of important events in this chapter\"],\n",
    "            \"characters_present\": [\"List of characters in this chapter\"],\n",
    "            \"setting\": \"Where this chapter takes place\",\n",
    "            \"mood\": \"Emotional mood of this chapter\"\n",
    "          }\n",
    "        ],\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    CRITICAL REQUIREMENTS:\n",
    "    1. Output ONLY valid JSON - no additional text, explanations, or markdown\n",
    "    2. Do not wrap JSON in json blocks\n",
    "    3. Include ALL required fields even if empty (use empty arrays [] or empty strings \"\")\n",
    "    4. Create logical chapters based on visual transitions and content changes\n",
    "    5. Reference specific frames using [row√ócol] notation and exact timestamps\n",
    "    6. Make chapters meaningful segments of 10-30 seconds each when possible\n",
    "    7. Do not add any markdown syntax for json output.\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    return prompt\n",
    "\n",
    "def analyze_with_bedrock(image_paths, prompt):\n",
    "    content = []\n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        content.append({\n",
    "            'type': 'image',\n",
    "            'source': {\n",
    "                'type': 'base64',\n",
    "                'media_type': 'image/jpeg',\n",
    "                'data': encode_image_to_base64(img_path)\n",
    "            }\n",
    "        })\n",
    "        content.append({'type': 'text', 'text': f'--- Grid {i+1} ---'})\n",
    "    \n",
    "    content.append({'type': 'text', 'text': prompt})\n",
    "\n",
    "    bedrock_runtime = boto3.client(\n",
    "        service_name='bedrock-runtime',\n",
    "        region_name=AWS_REGION\n",
    "    )\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=CLAUDE_MODEL_ID,\n",
    "        body=json.dumps({\n",
    "            'anthropic_version': 'bedrock-2023-05-31',\n",
    "            'max_tokens': 4096,\n",
    "            'messages': [{'role': 'user', 'content': content}]\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    return json.loads(response['body'].read())['content'][0]['text']\n",
    "\n",
    "print('‚úÖ Functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Analyze Video with Claude\n",
    "\n",
    "**Send the video filmstrip grids to Claude for analysis** - This is where the magic happens!\n",
    "\n",
    "### What Happens Now\n",
    "\n",
    "We're about to send the organized video grids to Claude AI on Amazon Bedrock for analysis. Here's the process:\n",
    "\n",
    "1. **Select Grids**: Choose the first few grids (like 5 grids = 100 video frames)\n",
    "2. **Add Context**: Include information about scene changes we detected\n",
    "3. **Create Instructions**: Tell Claude how to read the grid format and also sequence of grids reading.\n",
    "4. **Send to AI**: Upload everything to Claude on Amazon Bedrock\n",
    "5. **Get Analysis**: Receive detailed insights about the video\n",
    "\n",
    "### What Claude Will Tell You:\n",
    "- **Video Summary**: What the video is about overall\n",
    "- **Story Flow**: How the narrative develops over time\n",
    "- **Key Scenes**: Important moments with specific timestamps\n",
    "- **Visual Style**: Cinematography and visual elements\n",
    "- **Shot Changes**: How the detected transitions fit the story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids_to_analyze = output_files[:MAX_GRIDS_TO_ANALYZE]\n",
    "\n",
    "# Filter shot changes for grids being analyzed\n",
    "filtered_shot_changes = [sc for sc in shot_changes_info if sc['grid_index'] < MAX_GRIDS_TO_ANALYZE]\n",
    "\n",
    "# Create prompt with shot change information\n",
    "prompt = create_analysis_prompt(\n",
    "    FIXED_GRID_ROWS, \n",
    "    FIXED_GRID_COLS, \n",
    "    len(grids_to_analyze),\n",
    "    shot_changes_info=filtered_shot_changes if ENABLE_SHOT_DETECTION else None\n",
    ")\n",
    "\n",
    "print(f'ü§ñ Analyzing {len(grids_to_analyze)} grids...')\n",
    "if ENABLE_SHOT_DETECTION:\n",
    "    total_shots_in_analysis = sum(len(sc['shot_segments']) for sc in filtered_shot_changes)\n",
    "    print(f'   Including {total_shots_in_analysis} shot changes')\n",
    "\n",
    "analysis = analyze_with_bedrock(grids_to_analyze, prompt)\n",
    "print('‚úÖ Analysis complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Display Analysis Results\n",
    "\n",
    "**Format the JSON analysis into a user-friendly collapsible display**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('components')\n",
    "from display_utils import display_analysis_results\n",
    "\n",
    "# Display the analysis results with video clips\n",
    "display_analysis_results(analysis, VIDEO_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Review Video Analysis Raw Output from Model\n",
    "\n",
    "**See what Claude has discovered after analyzing the video** - Review the AI's understanding of the video content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*80)\n",
    "print('CLAUDE ANALYSIS')\n",
    "print('='*80)\n",
    "print()\n",
    "print(analysis)\n",
    "print()\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéâ Congratulations! You now understand how to perform visual understanding at scale with AI!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
