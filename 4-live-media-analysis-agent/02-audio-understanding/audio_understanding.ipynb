{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Understanding\n",
    "\n",
    "## üéØ AI-Powered Audio Understanding with Transcription & Spectrogram Analysis\n",
    "\n",
    "### What is Audio Understanding in Video?\n",
    "\n",
    "Audio understanding means teaching AI to \"hear\" and comprehend spoken content just like humans do. When you listen to a video, you naturally understand:\n",
    "- **What's being said** - the actual words and sentences\n",
    "- **How it sounds** - the tone, tempo, energy, and emotional characteristics\n",
    "- **Context** - how the conversation flows\n",
    "- **Key topics** - the main subjects being discussed\n",
    "- **Timing** - when important things are said\n",
    "\n",
    "### Live Stream Audio Understanding\n",
    "\n",
    "Now imagine applying audio understanding to **live streams** - audio content that's happening in real-time. This could be:\n",
    "\n",
    "- **Live broadcasts** - news, interviews, podcasts happening now\n",
    "- **Video calls** - meetings, presentations, conversations\n",
    "- **Streaming content** - live shows, tutorials, commentary\n",
    "- **Events** - conferences, speeches, announcements\n",
    "\n",
    "Live stream audio understanding means AI can transcribe and analyze both spoken content and acoustic characteristics **as it happens**, providing real-time insights about what's being said and how it sounds. This opens up powerful possibilities like automatic captions, emotion detection, content moderation, key moment detection, and live content summarization.\n",
    "\n",
    "### Our Solution: Real-Time Audio Processing Pipeline\n",
    "\n",
    "We address these challenges with a smart approach:\n",
    "\n",
    "- **üé§ Audio Extraction**: Extract audio streams from video content\n",
    "- **üìù Real-Time Transcription**: Use Amazon Transcribe for real-time speech-to-text\n",
    "- **üî§ Smart Sentence Building**: Combine words into complete sentences with timing\n",
    "- **üìä Spectrogram Analysis**: Extract acoustic features like tempo, energy, and frequency characteristics\n",
    "- **üß† AI Content Analysis**: Use Claude to organize content into chapters and topics with audio insights\n",
    "\n",
    "**Let's see how this works in practice!** üéß"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "**Load required modules** for audio processing, transcription, and AI analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import io\n",
    "import wave\n",
    "from pathlib import Path\n",
    "from IPython.display import Audio, display, HTML, JSON\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "from amazon_transcribe.client import TranscribeStreamingClient\n",
    "from amazon_transcribe.handlers import TranscriptResultStreamHandler\n",
    "from amazon_transcribe.model import TranscriptEvent\n",
    "sample_rate=16000\n",
    "\n",
    "# Load shared configuration from prerequisites notebook\n",
    "%store -r AUDIO_MODEL_ID\n",
    "%store -r AWS_REGION\n",
    "\n",
    "stream_duration = 120  # Process 120 seconds\n",
    "\n",
    "AUDIO_MODEL_ID=\"global.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preview Source Audio\n",
    "\n",
    "**Listen to the audio** we'll analyze for transcription and spectrogram features.\n",
    "\n",
    "### Source Video\n",
    "**Meridian, 2016**, Mystery from [Netflix](https://opencontent.netflix.com/#h.fzfk5hndrb9w) - This video is available under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode)\n",
    "\n",
    "We'll extract and analyze 2-minute audio track from the video for both spoken content and acoustic characteristics.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Using the same video source as visual understanding allows you to see how audio and visual analysis complement each other.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from IPython.display import HTML, display\n",
    "import IPython\n",
    "import base64\n",
    "\n",
    "def create_audio_from_video(video_path, start_time=0, duration=120):\n",
    "    \"\"\"Extract audio from video and create MP3 file with playback\"\"\"\n",
    "    \n",
    "    # Create output MP3 file\n",
    "    output_path = \"extracted_audio.mp3\"\n",
    "    \n",
    "    try:\n",
    "        # Extract audio using FFmpeg to MP3\n",
    "        ffmpeg_command = [\n",
    "            'ffmpeg', '-y',\n",
    "            '-ss', str(start_time),\n",
    "            '-i', video_path,\n",
    "            '-t', str(duration),\n",
    "            '-vn',\n",
    "            '-acodec', 'mp3',\n",
    "            '-ab', '128k',\n",
    "            output_path\n",
    "        ]\n",
    "        \n",
    "        print(f\"üéµ Extracting audio: {start_time}s to {start_time + duration}s\")\n",
    "        result = subprocess.run(ffmpeg_command, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            print(f\"‚ùå Error extracting audio: {result.stderr}\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"‚úÖ Audio extracted successfully: {output_path}\")\n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Video configuration\n",
    "video_path = \"../sample_videos/Netflix_Open_Content_Meridian.mp4\"\n",
    "start_time = 0  # Start from beginning\n",
    "duration = 120  # Extract 2 minutes\n",
    "\n",
    "# Extract and display audio\n",
    "audio_file = create_audio_from_video(video_path, start_time, duration)\n",
    "IPython.display.Audio(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Audio Stream From Source Video\n",
    "\n",
    "**Set up audio extraction** from the video file using FFmpeg to simulate real-time streaming.\n",
    "\n",
    "### Audio Processing Setup\n",
    "\n",
    "- **Real-time streaming** using FFmpeg with `-re` flag\n",
    "- **16kHz sample rate** optimized for speech recognition\n",
    "- **Mono audio** to reduce processing overhead\n",
    "- **PCM format** for direct streaming to Amazon Transcribe\n",
    "\n",
    "This creates a continuous audio stream that mimics live broadcast conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize FFmpeg audio streaming** from the video file with real-time processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def cleanup_ffmpeg_processes():\n",
    "    \"\"\"Kill any existing FFmpeg processes to prevent conflicts\"\"\"\n",
    "    import subprocess\n",
    "    import signal\n",
    "    import os\n",
    "    \n",
    "    try:\n",
    "        # Find all FFmpeg processes\n",
    "        result = subprocess.run(\n",
    "            ['pgrep', '-f', 'ffmpeg'],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0 and result.stdout.strip():\n",
    "            pids = result.stdout.strip().split('\\n')\n",
    "            print(f\"üßπ Found {len(pids)} existing FFmpeg process(es), cleaning up...\")\n",
    "            \n",
    "            for pid in pids:\n",
    "                try:\n",
    "                    pid_int = int(pid)\n",
    "                    os.kill(pid_int, signal.SIGTERM)\n",
    "                    print(f\"   ‚úÖ Terminated FFmpeg process (PID: {pid_int})\")\n",
    "                except (ValueError, ProcessLookupError) as e:\n",
    "                    print(f\"   ‚ö†Ô∏è Could not terminate PID {pid}: {e}\")\n",
    "            \n",
    "            # Wait a moment for processes to terminate\n",
    "            await asyncio.sleep(1)\n",
    "            print(\"üßπ FFmpeg cleanup complete\")\n",
    "        else:\n",
    "            print(\"‚úÖ No existing FFmpeg processes found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error during FFmpeg cleanup: {e}\")\n",
    "\n",
    "async def stream_audio_from_video(video_file, start_time, duration, sample_rate=16000):\n",
    "    \"\"\"Stream audio directly from video file using FFmpeg - with cleanup\"\"\"\n",
    "    \n",
    "    # Clean up any existing FFmpeg processes first\n",
    "    await cleanup_ffmpeg_processes()\n",
    "    \n",
    "    # FFmpeg command for real-time audio streaming\n",
    "    ffmpeg_command = [\n",
    "        'ffmpeg',\n",
    "        '-re',                    # Real-time flag - crucial for proper streaming\n",
    "        '-ss', str(start_time),   # Start time\n",
    "        '-i', video_file,         # Input video\n",
    "        '-t', str(duration),      # Duration\n",
    "        '-tune', 'zerolatency',   # Low latency tuning\n",
    "        '-f', 'wav',              # WAV format output\n",
    "        '-ac', '1',               # Mono\n",
    "        '-ar', str(sample_rate),  # Sample rate\n",
    "        '-c:a', 'pcm_s16le',      # 16-bit PCM\n",
    "        '-'                       # Output to stdout\n",
    "    ]\n",
    "    \n",
    "    print(f\"üéµ Starting FFmpeg audio stream: {start_time}s to {start_time + duration}s\")\n",
    "    print(\"üìä Using real-time streaming with -re flag\")\n",
    "    \n",
    "    # Start FFmpeg process\n",
    "    process = await asyncio.create_subprocess_exec(\n",
    "        *ffmpeg_command, \n",
    "        stdout=asyncio.subprocess.PIPE, \n",
    "        stderr=asyncio.subprocess.PIPE\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ FFmpeg process started, ready for streaming\")\n",
    "    return process\n",
    "\n",
    "# Start FFmpeg streaming process\n",
    "ffmpeg_process = await stream_audio_from_video(video_path, start_time, stream_duration, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Real-Time Transcription Processor\n",
    "\n",
    "**Set up the transcription processor** to process real-time audio stream through Amazon Transcribe for instant speech-to-text conversion.\n",
    "\n",
    "### Smart Sentence Building\n",
    "\n",
    "The transcription handler:\n",
    "- **Buffers incoming words** from the streaming API\n",
    "- **Detects sentence boundaries** using punctuation markers\n",
    "- **Combines comma-separated phrases** into complete sentences\n",
    "- **Tracks precise timing** for each sentence start and end\n",
    "- **Handles real-time processing** of partial and final results\n",
    "\n",
    "This creates complete, timestamped sentences from continuous speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SentenceBuilder and TranscriptProcessor for handling transcription processing\n",
    "# üìÅ Implementation: sentence_builder.py - handles sentence building and comma combination logic\n",
    "# üìÅ Implementation: transcript_processor.py - handles complex transcript processing operations\n",
    "from components import SentenceBuilder, TranscriptItemProcessor, SentenceFormatter, TranscriptEventValidator\n",
    "\n",
    "print(\"‚úÖ SentenceBuilder and TranscriptProcessor imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscriptionHandler(TranscriptResultStreamHandler):\n",
    "    \"\"\"Restored handler with proper sentence building logic\"\"\"\n",
    "    def __init__(self, transcript_result_stream):\n",
    "        super().__init__(transcript_result_stream)\n",
    "        self.sentences = []\n",
    "        self.sentence_builder = SentenceBuilder()\n",
    "        # Restore essential buffering for proper sentence formation\n",
    "        self.partial_buffer = {}\n",
    "        self.last_processed_stable_key = None\n",
    "        self.sentence_processed_keys = set()\n",
    "    \n",
    "    def _create_sentence_from_buffer(self):\n",
    "        \"\"\"Build complete sentences from buffered word items with comma handling\"\"\"\n",
    "        if not self.partial_buffer:\n",
    "            return\n",
    "        \n",
    "        sorted_items = sorted(self.partial_buffer.values(), key=lambda x: x.start_time)\n",
    "        sentence_words = []\n",
    "        sentence_start = None\n",
    "        sentence_end = None\n",
    "        punctuation = \"\"\n",
    "        items_to_remove = []\n",
    "        \n",
    "        for item in sorted_items:\n",
    "            item_key = f\"{item.start_time}_{item.end_time}_{item.content}\"\n",
    "            \n",
    "            if item.item_type == \"pronunciation\":\n",
    "                sentence_words.append(item.content)\n",
    "                if sentence_start is None:\n",
    "                    sentence_start = item.start_time\n",
    "                sentence_end = item.end_time\n",
    "                items_to_remove.append(item_key)\n",
    "            elif item.item_type == \"punctuation\":\n",
    "                punctuation = item.content.strip()\n",
    "                items_to_remove.append(item_key)\n",
    "                break\n",
    "        \n",
    "        if sentence_words:\n",
    "            # Use SentenceBuilder for comma handling\n",
    "            sentence_data = self.sentence_builder.add_sentence_fragment(\n",
    "                sentence_words, sentence_start, sentence_end, punctuation\n",
    "            )\n",
    "            \n",
    "            if sentence_data:\n",
    "                self._finalize_sentence(sentence_data)\n",
    "            \n",
    "            # Clean up processed items\n",
    "            for key in items_to_remove:\n",
    "                if key in self.partial_buffer:\n",
    "                    del self.partial_buffer[key]\n",
    "                    self.sentence_processed_keys.add(key)\n",
    "    \n",
    "    def _finalize_sentence(self, sentence_data):\n",
    "        \"\"\"Output a completed sentence\"\"\"\n",
    "        if sentence_data:\n",
    "            timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(f\"üìù SENTENCE: {sentence_data['text']}\")\n",
    "            print(f\"‚è±Ô∏è Time: {sentence_data['start_time']:.3f}s-{sentence_data['end_time']:.3f}s\")\n",
    "            \n",
    "            self.sentences.append({\n",
    "                'text': sentence_data['text'],\n",
    "                'start_time': sentence_data['start_time'],\n",
    "                'end_time': sentence_data['end_time'],\n",
    "                'timestamp': timestamp\n",
    "            })\n",
    "    \n",
    "    async def handle_transcript_event(self, transcript_event: TranscriptEvent):\n",
    "        \"\"\"Restored transcript processing with proper partial and final result handling\"\"\"\n",
    "        try:\n",
    "            if not transcript_event or not hasattr(transcript_event, 'transcript'):\n",
    "                return\n",
    "            \n",
    "            if not transcript_event.transcript or not hasattr(transcript_event.transcript, 'results'):\n",
    "                return\n",
    "                \n",
    "            results = transcript_event.transcript.results\n",
    "            if not results:\n",
    "                return\n",
    "            \n",
    "            for result in results:\n",
    "                if not result or not hasattr(result, 'alternatives') or not result.alternatives:\n",
    "                    continue\n",
    "                    \n",
    "                alt = result.alternatives[0]\n",
    "                if not alt:\n",
    "                    continue\n",
    "                \n",
    "                if result.is_partial:\n",
    "                    # Process partial results for real-time sentence building\n",
    "                    if hasattr(alt, 'items') and alt.items:\n",
    "                        found_last_processed = self.last_processed_stable_key is None\n",
    "                        \n",
    "                        for item in alt.items:\n",
    "                            if not item or not hasattr(item, 'item_type'):\n",
    "                                continue\n",
    "                                \n",
    "                            if not all(hasattr(item, attr) for attr in ['start_time', 'end_time', 'content']):\n",
    "                                continue\n",
    "                                \n",
    "                            item_key = f\"{item.start_time}_{item.end_time}_{item.content}\"\n",
    "                            \n",
    "                            if not found_last_processed:\n",
    "                                if item_key == self.last_processed_stable_key:\n",
    "                                    found_last_processed = True\n",
    "                                continue\n",
    "                            \n",
    "                            if hasattr(item, 'stable') and item.stable:\n",
    "                                if item_key not in self.partial_buffer:\n",
    "                                    self.partial_buffer[item_key] = item\n",
    "                                    self.last_processed_stable_key = item_key\n",
    "                                    \n",
    "                                    if item.item_type == \"punctuation\":\n",
    "                                        self._create_sentence_from_buffer()\n",
    "                else:\n",
    "                    # Process final results\n",
    "                    if hasattr(alt, 'items') and alt.items:\n",
    "                        for item in alt.items:\n",
    "                            if not item or not hasattr(item, 'item_type'):\n",
    "                                continue\n",
    "                                \n",
    "                            if not all(hasattr(item, attr) for attr in ['start_time', 'end_time', 'content']):\n",
    "                                continue\n",
    "                                \n",
    "                            item_key = f\"{item.start_time}_{item.end_time}_{item.content}\"\n",
    "                            \n",
    "                            if item_key in self.sentence_processed_keys:\n",
    "                                continue\n",
    "                            \n",
    "                            if item_key not in self.partial_buffer:\n",
    "                                self.partial_buffer[item_key] = item\n",
    "                                \n",
    "                                if item.item_type == \"punctuation\":\n",
    "                                    self._create_sentence_from_buffer()\n",
    "                    \n",
    "                    # Clean up processed items\n",
    "                    remaining_items = {k: v for k, v in self.partial_buffer.items() if k not in self.sentence_processed_keys}\n",
    "                    self.partial_buffer = remaining_items\n",
    "                    self.sentence_processed_keys.clear()\n",
    "                    \n",
    "                    if not remaining_items:\n",
    "                        self.last_processed_stable_key = None\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Transcript event error: {e}\")\n",
    "    \n",
    "    def finalize_sentences(self):\n",
    "        \"\"\"Force completion of any pending sentences at end of stream\"\"\"\n",
    "        sentence_data = self.sentence_builder.finalize_pending()\n",
    "        if sentence_data:\n",
    "            print(f\"üìù FINAL SENTENCE: {sentence_data['text']}\")\n",
    "            print(f\"‚è±Ô∏è Time: {sentence_data['start_time']:.3f}s\")\n",
    "            timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            self.sentences.append({\n",
    "                'text': sentence_data['text'],\n",
    "                'start_time': sentence_data['start_time'],\n",
    "                'end_time': sentence_data['end_time'],\n",
    "                'timestamp': timestamp\n",
    "            })\n",
    "print(\"‚úÖ Transcription Handler initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Execute Real-Time Transcription Processor\n",
    "\n",
    "**Start the real-time transcription process** - Stream audio to Amazon Transcribe and build complete sentences.\n",
    "\n",
    "### What Happens Now\n",
    "\n",
    "1. **Initialize Transcribe client** with English language settings\n",
    "2. **Start streaming audio** from FFmpeg to Amazon Transcribe\n",
    "3. **Process transcription results** in real-time\n",
    "4. **Build complete sentences** with precise timestamps\n",
    "5. **Display results** as they are detected\n",
    "\n",
    "You'll see sentences appear like:\n",
    "- üìù SENTENCE: That's right.\n",
    "- ‚è±Ô∏è Time: 4.63s - 5.14s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Transcribe client and handler (like working notebook)\n",
    "print(\"üîß Setting up Transcribe client...\")\n",
    "\n",
    "transcribe_client = TranscribeStreamingClient(region=AWS_REGION)\n",
    "\n",
    "transcribe_stream = await transcribe_client.start_stream_transcription(\n",
    "    language_code='en-US',\n",
    "    media_sample_rate_hz=sample_rate,\n",
    "    media_encoding='pcm'\n",
    ")\n",
    "\n",
    "# Setup handler\n",
    "handler = TranscriptionHandler(transcribe_stream.output_stream)\n",
    "asyncio.create_task(handler.handle_events())\n",
    "\n",
    "print(\"‚úÖ Transcribe stream ready\")\n",
    "print(\"üì° Starting real-time FFmpeg ‚Üí Transcribe streaming...\")\n",
    "\n",
    "# Real-time streaming loop (based on working notebook)\n",
    "try:\n",
    "    chunks_sent = 0\n",
    "    total_bytes = 0\n",
    "    last_data_time = time.time()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Read audio data from FFmpeg stdout (like working notebook)\n",
    "            data = await asyncio.wait_for(ffmpeg_process.stdout.read(1024 * 2), timeout=1.0)\n",
    "            \n",
    "            if data:\n",
    "                last_data_time = time.time()\n",
    "                \n",
    "                # Send directly to Transcribe\n",
    "                await transcribe_stream.input_stream.send_audio_event(audio_chunk=data)\n",
    "                \n",
    "                chunks_sent += 1\n",
    "                total_bytes += len(data)\n",
    "                \n",
    "                # Progress every 50 chunks (~2 seconds)\n",
    "                if chunks_sent % 50 == 0:\n",
    "                    elapsed = total_bytes / (sample_rate * 2)\n",
    "                    #print(f\"üìä Streamed {elapsed:.1f}s ({chunks_sent} chunks)\")\n",
    "            else:\n",
    "                print(\"üì° FFmpeg stream ended\")\n",
    "                break\n",
    "                \n",
    "        except asyncio.TimeoutError:\n",
    "            time_since_data = time.time() - last_data_time\n",
    "            if time_since_data >= 10:  # 10 second timeout\n",
    "                print(\"‚ö†Ô∏è No audio data for 10 seconds, stopping...\")\n",
    "                break\n",
    "    \n",
    "    # End streams\n",
    "    await transcribe_stream.input_stream.end_stream()\n",
    "    await ffmpeg_process.wait()\n",
    "    \n",
    "    # Finalize any pending sentences\n",
    "    handler.finalize_sentences()\n",
    "    \n",
    "    # Final stats\n",
    "    total_seconds = total_bytes / (sample_rate * 2)\n",
    "    print(f\"\\n‚úÖ Streaming complete: {total_seconds:.1f}s ({chunks_sent} chunks)\")\n",
    "    print(f\"üìù Sentences detected: {len(handler.sentences)}\")\n",
    "    \n",
    "    # Final cleanup to ensure no processes remain\n",
    "    await cleanup_ffmpeg_processes()\n",
    "    \n",
    "    # Wait for final transcription\n",
    "    await asyncio.sleep(3)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Streaming error: {e}\")\n",
    "    if ffmpeg_process.returncode is None:\n",
    "        ffmpeg_process.terminate()\n",
    "        await ffmpeg_process.wait()\n",
    "    \n",
    "    # Cleanup on error as well\n",
    "    await cleanup_ffmpeg_processes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Display Consolidated Transcript\n",
    "\n",
    "**Review the complete transcription** results from the real-time processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create buffer and display transcript\n",
    "sentence_json = [{'sentence': s['text'], 'start_time': s['start_time'], 'end_time': s['end_time']} \n",
    "                       for s in handler.sentences] if handler.sentences else []\n",
    "\n",
    "print(f\"\\nüìö TRANSCRIPT ({len(sentence_json)} sentences)\")\n",
    "print(\"=\" * 50)\n",
    "if sentence_json:\n",
    "    print(\" \".join(s['sentence'] for s in sentence_json))\n",
    "    print(f\"üìä {sentence_json[0]['start_time']:.1f}s - {sentence_json[-1]['end_time']:.1f}s\")\n",
    "else:\n",
    "    print(\"No sentences detected\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perform Audio Feature Extraction\n",
    "\n",
    "**Extract audio characteristics** to enhance content analysis with acoustic insights.\n",
    "\n",
    "### Audio Features Extracted\n",
    "\n",
    "- **Spectral Centroid** - Audio \"brightness\" measurement\n",
    "- **RMS Energy** - Overall loudness and dynamic range\n",
    "- **Zero Crossing Rate** - Speech vs music distinction\n",
    "- **Tempo Detection** - Rhythmic patterns in speech\n",
    "- **MFCC Features** - Speech analysis coefficients\n",
    "- **Spectral Rolloff** - Frequency distribution characteristics\n",
    "\n",
    "These features provide additional context that complements the transcribed text for better AI analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import AudioSpectrogramAnalyzer for comprehensive audio analysis\n",
    "# üìÅ Implementation: audio_spectrogram_analyzer.py - handles mel-spectrogram generation, waveform visualization, and comprehensive audio feature extraction\n",
    "from components import AudioSpectrogramAnalyzer\n",
    "\n",
    "print(\"‚úÖ AudioSpectrogramAnalyzer imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive spectrogram analysis and extract audio features for AI model enhancement\n",
    "print(\"üéµ Starting spectrogram analysis...\")\n",
    "\n",
    "# Initialize the analyzer\n",
    "spectrogram_analyzer = AudioSpectrogramAnalyzer(sample_rate=sample_rate)\n",
    "\n",
    "# Extract audio from video for analysis\n",
    "audio_data, sr = spectrogram_analyzer.extract_audio_from_video(\n",
    "    video_path, \n",
    "    start_time=start_time, \n",
    "    duration=stream_duration\n",
    ")\n",
    "\n",
    "if audio_data is not None:\n",
    "    # Generate mel-spectrogram for frequency analysis\n",
    "    mel_spec = spectrogram_analyzer.generate_spectrogram(audio_data, sr)\n",
    "    \n",
    "    # Extract comprehensive audio features that will be provided to Amazon Bedrock\n",
    "    audio_features = spectrogram_analyzer.analyze_audio_features(audio_data, sr)\n",
    "    \n",
    "    # Create detailed visualization (waveform, spectrogram, RMS energy)\n",
    "    spectrogram_image = spectrogram_analyzer.create_spectrogram_visualization(start_time_offset=start_time)\n",
    "    \n",
    "    # Generate human-readable audio characteristics description\n",
    "    audio_description = spectrogram_analyzer.get_audio_description()\n",
    "    \n",
    "    print(\"\\nüìä AUDIO ANALYSIS SUMMARY (Features for AI Model)\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üéµ Duration: {audio_features.get('duration', 0):.1f}s\")\n",
    "    print(f\"üéº Tempo: {audio_features.get('tempo', 0):.1f} BPM (rhythmic patterns)\")\n",
    "    print(f\"üìà Spectral Centroid: {audio_features.get('spectral_centroid_mean', 0):.1f} Hz (audio brightness)\")\n",
    "    print(f\"üîä RMS Energy: {audio_features.get('rms_mean', 0):.4f} (loudness/dynamics)\")\n",
    "    print(f\"üéôÔ∏è Zero Crossing Rate: {audio_features.get('zero_crossing_rate_mean', 0):.4f} (speech vs music)\")\n",
    "    print(f\"üìù Audio Characteristics: {audio_description}\")\n",
    "    print(\"\\nüí° These features will be provided to Amazon Bedrock for enhanced content analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Could not extract audio for spectrogram analysis\")\n",
    "    audio_features = {}\n",
    "    spectrogram_image = None\n",
    "    audio_description = \"Audio analysis unavailable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Define Audio Content Analyzer Leveraging Claude\n",
    "\n",
    "**Define the enhanced audio content analyzer** to analyze transcribed content with audio insights using Claude for creating structured chapters and topics.\n",
    "\n",
    "This transforms raw transcription and audio insights into organized chapters and topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced AudioContentAnalyzer with spectrogram integration\n",
    "class EnhancedAudioContentAnalyzer:\n",
    "    \"\"\"Enhanced analyzer that includes spectrogram data in content analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, model_id=None, region=None):\n",
    "        self.bedrock_client = boto3.client('bedrock-runtime', region_name=region or AWS_REGION)\n",
    "        self.model_id = model_id or AUDIO_MODEL_ID\n",
    "    \n",
    "    def create_analysis_prompt(self, sentences, audio_features=None, audio_description=None):\n",
    "        \"\"\"Create a structured prompt for chapter and topic analysis with spectrogram data\"\"\"\n",
    "        transcript_text = \" \".join([s['sentence'] for s in sentences])\n",
    "        \n",
    "        # Build audio analysis section if available\n",
    "        audio_analysis_section = \"\"\n",
    "        if audio_features and audio_description:\n",
    "            audio_analysis_section = f\"\"\"\n",
    "\n",
    "            AUDIO SPECTROGRAM ANALYSIS:\n",
    "            Duration: {audio_features.get('duration', 0):.1f} seconds\n",
    "            Tempo: {audio_features.get('tempo', 0):.1f} BPM\n",
    "            Spectral Centroid: {audio_features.get('spectral_centroid_mean', 0):.1f} Hz\n",
    "            RMS Energy: {audio_features.get('rms_mean', 0):.4f}\n",
    "            Zero Crossing Rate: {audio_features.get('zero_crossing_rate_mean', 0):.4f}\n",
    "            Audio Characteristics: {audio_description}\n",
    "            \n",
    "            MFCC Features (Speech Analysis):\n",
    "            {json.dumps(audio_features.get('mfcc_means', [])[:5], indent=2)}  # First 5 MFCC coefficients\n",
    "            \n",
    "            AUDIO INSIGHTS FOR CONTENT ANALYSIS:\n",
    "            - Spectral Centroid ({audio_features.get('spectral_centroid_mean', 0):.1f} Hz): {'High energy/animated' if audio_features.get('spectral_centroid_mean', 0) > 2000 else 'Calm/conversational'} delivery\n",
    "            - RMS Energy ({audio_features.get('rms_mean', 0):.4f}): {'Dynamic/emphatic' if audio_features.get('rms_mean', 0) > 0.1 else 'Steady/measured'} speaking style\n",
    "            - Zero Crossing Rate ({audio_features.get('zero_crossing_rate_mean', 0):.4f}): {'Clear speech focus' if audio_features.get('zero_crossing_rate_mean', 0) > 0.1 else 'Background elements present'}\n",
    "            - Tempo ({audio_features.get('tempo', 0):.1f} BPM): {'Rapid/urgent' if audio_features.get('tempo', 0) > 120 else 'Deliberate/thoughtful'} pacing\n",
    "            - MFCC patterns indicate speech clarity and vocal characteristics\"\"\"\n",
    "            \n",
    "        prompt = f\"\"\"Analyze the following transcript and organize it into chapters and topics. MANDATORY: Incorporate the audio spectrogram analysis into your chapter titles, summaries, and descriptions to reflect the speaker's delivery style and emotional tone.\n",
    "        \n",
    "        TRANSCRIPT:\n",
    "        {transcript_text}\n",
    "        \n",
    "        SENTENCE TIMING DATA:\n",
    "        {json.dumps(sentences, indent=2)}{audio_analysis_section}\n",
    "        \n",
    "        Please create a structured analysis with the following format:\n",
    "        \n",
    "        {{\n",
    "          \"chapters\": [\n",
    "            {{\n",
    "              \"title\": \"Chapter Title (must reflect audio energy/tone)\",\n",
    "              \"start_time\": 0.0,\n",
    "              \"end_time\": 30.0,\n",
    "              \"summary\": \"Brief chapter summary incorporating audio delivery characteristics\",\n",
    "              \"audio_tone\": \"Description of speaker's energy and delivery style in this section\",\n",
    "              \"topics\": [\n",
    "                {{\n",
    "                  \"title\": \"Topic Title\",\n",
    "                  \"start_time\": 0.0,\n",
    "                  \"end_time\": 15.0,\n",
    "                  \"description\": \"Topic description enhanced with audio context\",\n",
    "                  \"key_points\": [\"Point 1\", \"Point 2\"]\n",
    "                }}\n",
    "              ]\n",
    "            }}\n",
    "          ],\n",
    "          \"overall_summary\": \"Overall content summary that includes speaker's delivery style and audio characteristics\",\n",
    "          \"audio_delivery_analysis\": \"Summary of how the speaker's vocal patterns and energy levels enhance the content\",\n",
    "          \"total_duration\": 120.0\n",
    "        }}\n",
    "        \n",
    "        CRITICAL REQUIREMENTS:\n",
    "        - Chapter titles MUST include descriptors based on audio energy (e.g., \"Energetic Opening\", \"Measured Technical Discussion\")\n",
    "        - All summaries MUST reference the speaker's delivery style using the spectrogram data\n",
    "        - Include \"audio_tone\" field for each chapter describing vocal characteristics\n",
    "        - Add \"audio_delivery_analysis\" field summarizing overall speaking patterns\n",
    "        - Use tempo to identify pacing changes between sections\n",
    "        - Use RMS energy to identify emphasis and key moments\n",
    "        - Use spectral centroid to gauge speaker engagement and excitement levels\n",
    "        - Descriptions should reflect whether content is delivered with high energy, calmly, urgently, etc.\n",
    "        - DONOT mention the audio charateristic measure numbers while you build the summary.\n",
    "        - MORE focus to the script delivered with inclusion of little emotions identified using audio analysis.\n",
    "        \n",
    "        Return only the JSON structure, no additional text.\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def analyze_content(self, sentences, audio_features=None, audio_description=None):\n",
    "        \"\"\"Send transcript to Bedrock for chapter and topic analysis with spectrogram data\"\"\"\n",
    "        if not sentences:\n",
    "            return {\"chapters\": [], \"overall_summary\": \"No content to analyze\", \"total_duration\": 0.0}\n",
    "        \n",
    "        prompt = self.create_analysis_prompt(sentences, audio_features, audio_description)\n",
    "        \n",
    "        try:\n",
    "            # Prepare the request for Claude\n",
    "            request_body = {\n",
    "                'anthropic_version': 'bedrock-2023-05-31',\n",
    "                'max_tokens': 4096,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": prompt\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                \"temperature\": 0.3\n",
    "            }\n",
    "            \n",
    "            print(\"ü§ñ Analyzing content with Amazon Bedrock (including spectrogram data)...\")\n",
    "            \n",
    "            response = self.bedrock_client.invoke_model(\n",
    "                modelId=self.model_id,\n",
    "                body=json.dumps(request_body)\n",
    "            )\n",
    "            \n",
    "            response_body = json.loads(response['body'].read())\n",
    "            analysis_text = response_body['content'][0]['text']\n",
    "            \n",
    "            # Parse the JSON response\n",
    "            try:\n",
    "                analysis = json.loads(analysis_text)\n",
    "                return analysis\n",
    "            except json.JSONDecodeError:\n",
    "                # If JSON parsing fails, extract JSON from the response\n",
    "                import re\n",
    "                json_match = re.search(r'\\{.*\\}', analysis_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    return json.loads(json_match.group())\n",
    "                else:\n",
    "                    raise ValueError(\"Could not parse JSON from response\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error analyzing content: {e}\")\n",
    "            return {\n",
    "                \"chapters\": [{\n",
    "                    \"title\": \"Content Analysis\",\n",
    "                    \"start_time\": sentences[0]['start_time'] if sentences else 0.0,\n",
    "                    \"end_time\": sentences[-1]['end_time'] if sentences else 0.0,\n",
    "                    \"summary\": \"Analysis failed - showing raw transcript\",\n",
    "                    \"topics\": [{\n",
    "                        \"title\": \"Transcript Content\",\n",
    "                        \"start_time\": sentences[0]['start_time'] if sentences else 0.0,\n",
    "                        \"end_time\": sentences[-1]['end_time'] if sentences else 0.0,\n",
    "                        \"description\": \" \".join([s['sentence'] for s in sentences[:5]]) + \"...\",\n",
    "                        \"key_points\": [\"Analysis unavailable\"]\n",
    "                    }]\n",
    "                }],\n",
    "                \"overall_summary\": f\"Content analysis failed: {str(e)}\",\n",
    "                \"total_duration\": sentences[-1]['end_time'] if sentences else 0.0\n",
    "            }\n",
    "\n",
    "print(\"‚úÖ Enhanced AudioContentAnalyzer with spectrogram integration ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Execute Audio Content Analysis\n",
    "\n",
    "**Analyze content using both transcription and audio features** using Claude to create comprehensive chapters and topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analyze the transcribed content with spectrogram data\n",
    "if sentence_json:\n",
    "    print(\"üîç Starting enhanced content analysis with spectrogram data...\")\n",
    "    \n",
    "    # Initialize the enhanced analyzer\n",
    "    analyzer = EnhancedAudioContentAnalyzer()\n",
    "    \n",
    "    # Perform the analysis with both transcript and spectrogram data\n",
    "    content_analysis = analyzer.analyze_content(sentence_json, audio_features, audio_description)\n",
    "    \n",
    "    print(\"\\nüìã ENHANCED CONTENT ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Display overall summary\n",
    "    print(f\"üìñ Overall Summary: {content_analysis.get('overall_summary', 'No summary available')}\")\n",
    "    print(f\"‚è±Ô∏è Total Duration: {content_analysis.get('total_duration', 0):.1f}s\")\n",
    "    print(f\"üìö Chapters Found: {len(content_analysis.get('chapters', []))}\")\n",
    "    \n",
    "    # Display chapters and topics\n",
    "    for i, chapter in enumerate(content_analysis.get('chapters', []), 1):\n",
    "        print(f\"\\nüìñ Chapter {i}: {chapter.get('title', 'Untitled')}\")\n",
    "        print(f\"   ‚è±Ô∏è Time: {chapter.get('start_time', 0):.1f}s - {chapter.get('end_time', 0):.1f}s\")\n",
    "        print(f\"   üìù Summary: {chapter.get('summary', 'No summary')}\")\n",
    "        \n",
    "        topics = chapter.get('topics', [])\n",
    "        print(f\"   üè∑Ô∏è Topics ({len(topics)}):\")\n",
    "        \n",
    "        for j, topic in enumerate(topics, 1):\n",
    "            print(f\"      {j}. {topic.get('title', 'Untitled Topic')}\")\n",
    "            print(f\"         ‚è±Ô∏è {topic.get('start_time', 0):.1f}s - {topic.get('end_time', 0):.1f}s\")\n",
    "            print(f\"         üìÑ {topic.get('description', 'No description')}\")\n",
    "            \n",
    "            key_points = topic.get('key_points', [])\n",
    "            if key_points:\n",
    "                print(f\"         üîë Key Points: {', '.join(key_points)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Store the analysis for further use\n",
    "    print(f\"\\n‚úÖ Enhanced analysis complete! Found {len(content_analysis.get('chapters', []))} chapters with detailed topics.\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No sentences available for analysis\")\n",
    "    content_analysis = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparison: With vs Without Spectrogram Analysis\n",
    "\n",
    "**See the difference** between basic transcript analysis and enhanced spectrogram-powered analysis.\n",
    "\n",
    "### Why Spectrogram Analysis Matters\n",
    "\n",
    "**Spectrogram analysis adds crucial acoustic context that transforms basic transcription into rich, nuanced understanding:**\n",
    "\n",
    "**üéØ Key Use Cases:**\n",
    "- **Key Moment Detection**: Identify excitement peaks, emphasis, and emotional highlights\n",
    "- **Speaker Engagement**: Detect when speakers are animated vs. calm/measured\n",
    "- **Content Pacing**: Understand rushed vs. deliberate delivery for better segmentation\n",
    "- **Emotional Context**: Capture tone that text alone cannot convey\n",
    "- **Quality Assessment**: Identify clear speech vs. background noise/music\n",
    "- **Audience Targeting**: Match content energy to appropriate audience segments\n",
    "- **Highlight Generation**: Create clips based on vocal energy and engagement levels\n",
    "\n",
    "**üìä Acoustic Features That Make a Difference:**\n",
    "- **Spectral Centroid**: Brightness/energy ‚Üí Excitement vs. calm delivery\n",
    "- **RMS Energy**: Loudness dynamics ‚Üí Emphasis and key moments\n",
    "- **Tempo**: Speech pacing ‚Üí Urgency vs. thoughtful discussion\n",
    "- **Zero Crossing Rate**: Speech clarity ‚Üí Professional vs. casual content\n",
    "- **MFCC**: Voice characteristics ‚Üí Speaker identification and emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a basic analyzer without spectrogram data for comparison\n",
    "from components import BasicAudioContentAnalyzer, ComparisonUIBuilder\n",
    "\n",
    "# Run comparison analysis with collapsible display\n",
    "if sentence_json:\n",
    "    # Basic analysis (transcript only)\n",
    "    basic_analyzer = BasicAudioContentAnalyzer(model_id=AUDIO_MODEL_ID, region=AWS_REGION)\n",
    "    basic_analysis = basic_analyzer.analyze_content(sentence_json)\n",
    "    \n",
    "    # Create UI builder and display comparison\n",
    "    ui_builder = ComparisonUIBuilder(video_path)\n",
    "    ui_builder.display_comparison(basic_analysis, content_analysis)\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No transcript available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéâ Congratulations! You now understand how to perform audio understanding with AI!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
